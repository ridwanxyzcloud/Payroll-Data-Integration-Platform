{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-25T13:52:59.198136Z",
     "start_time": "2024-08-25T13:52:59.181097Z"
    }
   },
   "source": "import pandas as pd",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T18:59:33.784775Z",
     "start_time": "2024-08-26T18:59:32.464287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "\n",
    "import os\n",
    "from io import StringIO\n",
    "\n",
    "from helpers.metrics import start_metrics_server, files_extracted, rows_extracted, rows_transformed, rows_validated, missing_values_detected, rows_staged, fact_table_created, rows_processed, rows_cleaned, data_quality_issues \n",
    "from helpers.logging_utils import setup_logging\n",
    "\n",
    "setup_logging()\n",
    "load_dotenv(override=True)\n",
    "\n",
    "start_metrics_server(port=8001)"
   ],
   "id": "d1798b6ef16a075c",
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 48] Address already in use",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 14\u001B[0m\n\u001B[1;32m     11\u001B[0m setup_logging()\n\u001B[1;32m     12\u001B[0m load_dotenv(override\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m---> 14\u001B[0m \u001B[43mstart_metrics_server\u001B[49m\u001B[43m(\u001B[49m\u001B[43mport\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m8001\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/Payroll-Data-Integration-Platform/helpers/metrics.py:24\u001B[0m, in \u001B[0;36mstart_metrics_server\u001B[0;34m(port)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstart_metrics_server\u001B[39m(port\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8000\u001B[39m):\n\u001B[1;32m     18\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;124;03m    Starts a Prometheus metrics server.\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \n\u001B[1;32m     21\u001B[0m \u001B[38;5;124;03m    :param port: The port on which the metrics server will listen (default is 8000).\u001B[39;00m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;124;03m    :return: None\u001B[39;00m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m     \u001B[43mstart_http_server\u001B[49m\u001B[43m(\u001B[49m\u001B[43mport\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/Payroll-Data-Integration-Platform/.venv/lib/python3.9/site-packages/prometheus_client/exposition.py:221\u001B[0m, in \u001B[0;36mstart_wsgi_server\u001B[0;34m(port, addr, registry, certfile, keyfile, client_cafile, client_capath, protocol, client_auth_required)\u001B[0m\n\u001B[1;32m    219\u001B[0m TmpServer\u001B[38;5;241m.\u001B[39maddress_family, addr \u001B[38;5;241m=\u001B[39m _get_best_family(addr, port)\n\u001B[1;32m    220\u001B[0m app \u001B[38;5;241m=\u001B[39m make_wsgi_app(registry)\n\u001B[0;32m--> 221\u001B[0m httpd \u001B[38;5;241m=\u001B[39m \u001B[43mmake_server\u001B[49m\u001B[43m(\u001B[49m\u001B[43maddr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mport\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mapp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mTmpServer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhandler_class\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_SilentHandler\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    222\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m certfile \u001B[38;5;129;01mand\u001B[39;00m keyfile:\n\u001B[1;32m    223\u001B[0m     context \u001B[38;5;241m=\u001B[39m _get_ssl_ctx(certfile, keyfile, protocol, client_cafile, client_capath, client_auth_required)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/wsgiref/simple_server.py:154\u001B[0m, in \u001B[0;36mmake_server\u001B[0;34m(host, port, app, server_class, handler_class)\u001B[0m\n\u001B[1;32m    150\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmake_server\u001B[39m(\n\u001B[1;32m    151\u001B[0m     host, port, app, server_class\u001B[38;5;241m=\u001B[39mWSGIServer, handler_class\u001B[38;5;241m=\u001B[39mWSGIRequestHandler\n\u001B[1;32m    152\u001B[0m ):\n\u001B[1;32m    153\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Create a new WSGI server listening on `host` and `port` for `app`\"\"\"\u001B[39;00m\n\u001B[0;32m--> 154\u001B[0m     server \u001B[38;5;241m=\u001B[39m \u001B[43mserver_class\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhost\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mport\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhandler_class\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    155\u001B[0m     server\u001B[38;5;241m.\u001B[39mset_app(app)\n\u001B[1;32m    156\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m server\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/socketserver.py:452\u001B[0m, in \u001B[0;36mTCPServer.__init__\u001B[0;34m(self, server_address, RequestHandlerClass, bind_and_activate)\u001B[0m\n\u001B[1;32m    450\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m bind_and_activate:\n\u001B[1;32m    451\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 452\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mserver_bind\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    453\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mserver_activate()\n\u001B[1;32m    454\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m:\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/wsgiref/simple_server.py:50\u001B[0m, in \u001B[0;36mWSGIServer.server_bind\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mserver_bind\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m     49\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Override server_bind to store the server name.\"\"\"\u001B[39;00m\n\u001B[0;32m---> 50\u001B[0m     \u001B[43mHTTPServer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mserver_bind\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msetup_environ()\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/server.py:136\u001B[0m, in \u001B[0;36mHTTPServer.server_bind\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mserver_bind\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    135\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Override server_bind to store the server name.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 136\u001B[0m     \u001B[43msocketserver\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTCPServer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mserver_bind\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    137\u001B[0m     host, port \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mserver_address[:\u001B[38;5;241m2\u001B[39m]\n\u001B[1;32m    138\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mserver_name \u001B[38;5;241m=\u001B[39m socket\u001B[38;5;241m.\u001B[39mgetfqdn(host)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/socketserver.py:466\u001B[0m, in \u001B[0;36mTCPServer.server_bind\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    464\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mallow_reuse_address:\n\u001B[1;32m    465\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket\u001B[38;5;241m.\u001B[39msetsockopt(socket\u001B[38;5;241m.\u001B[39mSOL_SOCKET, socket\u001B[38;5;241m.\u001B[39mSO_REUSEADDR, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m--> 466\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msocket\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbind\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mserver_address\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    467\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mserver_address \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket\u001B[38;5;241m.\u001B[39mgetsockname()\n",
      "\u001B[0;31mOSError\u001B[0m: [Errno 48] Address already in use"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# EXTRACTION STAGE",
   "id": "dc58ae88a2a142ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T19:17:49.559252Z",
     "start_time": "2024-08-26T19:17:49.507670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from helpers.s3_utils import s3_client\n",
    "\n",
    "# File definitions\n",
    "master_files = ['EmpMaster.csv', 'TitleMaster.csv', 'AgencyMaster.csv']\n",
    "payroll_files = ['nycpayroll_2020.csv', 'nycpayroll_2021.csv']\n",
    "master_table_names = ['DimEmployee', 'DimTitle', 'DimAgency']\n",
    "dim_columns = [\n",
    "    ['EmployeeID', 'LastName', 'FirstName', 'LeaveStatusasofJune30'],\n",
    "    ['TitleCode', 'TitleDescription'],\n",
    "    ['AgencyID', 'AgencyName', 'AgencyStartDate']\n",
    "]\n",
    "\n",
    "# AWS and S3 configuration\n",
    "s3_bucket = os.getenv(\"s3_bucket\")\n",
    "s3_prefix = os.getenv(\"s3_prefix\")\n",
    "aws_region = os.getenv(\"aws_region\")\n",
    "aws_access_key_id = os.getenv(\"aws_access_key_id\")\n",
    "aws_secret_access_key = os.getenv(\"aws_secret_access_key\")\n",
    "\n",
    "\n",
    "def s3_client(aws_region, aws_access_key_id, aws_secret_access_key):\n",
    "    return boto3.client(\n",
    "        's3',\n",
    "        region_name=aws_region,\n",
    "        aws_access_key_id=aws_access_key_id,\n",
    "        aws_secret_access_key=aws_secret_access_key\n",
    "    )\n",
    "\n",
    "s3_client = s3_client(aws_region, aws_access_key_id, aws_secret_access_key)\n",
    "\n",
    "def extract_from_s3(s3_client, s3_bucket, s3_prefix, file_name):\n",
    "    try:\n",
    "        logging.info(f\"Extracting {file_name} from S3\")\n",
    "        obj = s3_client.get_object(Bucket=s3_bucket, Key=s3_prefix + file_name)\n",
    "        df = pd.read_csv(StringIO(obj['Body'].read().decode('utf-8')))\n",
    "\n",
    "        # Update metrics\n",
    "        files_extracted.inc()  # Increment the count of files extracted\n",
    "\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to extract {file_name}: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "def extract_data(file_name):\n",
    "    return extract_from_s3(s3_client, s3_bucket, s3_prefix, file_name)\n",
    "\n"
   ],
   "id": "57d3a035afd1c452",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T19:17:51.916847Z",
     "start_time": "2024-08-26T19:17:51.756069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_test = extract_data('TitleMaster.csv')\n",
    "df_test.head()"
   ],
   "id": "e11dfeb516df9b04",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 20:17:51,759 - root - INFO - Extracting TitleMaster.csv from S3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   TitleCode                TitleDescription\n",
       "0      40001  *ADM SCHOOL SECURITY MANAGER-U\n",
       "1      40002       *ADMIN SCHL SECUR MGR-MGL\n",
       "2      40003                *AGENCY ATTORNEY\n",
       "3      40004          *ASSISTANT ADVOCATE-PD\n",
       "4      40005    *ASSOCIATE EDUCATION OFFICER"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TitleCode</th>\n",
       "      <th>TitleDescription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40001</td>\n",
       "      <td>*ADM SCHOOL SECURITY MANAGER-U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40002</td>\n",
       "      <td>*ADMIN SCHL SECUR MGR-MGL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40003</td>\n",
       "      <td>*AGENCY ATTORNEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40004</td>\n",
       "      <td>*ASSISTANT ADVOCATE-PD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40005</td>\n",
       "      <td>*ASSOCIATE EDUCATION OFFICER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T14:45:35.957693Z",
     "start_time": "2024-08-25T14:45:35.946491Z"
    }
   },
   "cell_type": "code",
   "source": "len(df_test)",
   "id": "b2f03594a820a1b3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1446"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T14:45:49.508242Z",
     "start_time": "2024-08-25T14:45:49.501390Z"
    }
   },
   "cell_type": "code",
   "source": "new_df = df_test.drop_duplicates\n",
   "id": "94231d57ed5059f2",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T14:46:20.853379Z",
     "start_time": "2024-08-25T14:46:20.839635Z"
    }
   },
   "cell_type": "code",
   "source": "new_df",
   "id": "53066b8df7f34199",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.drop_duplicates of       TitleCode                                   TitleDescription\n",
       "0         40001                     *ADM SCHOOL SECURITY MANAGER-U\n",
       "1         40002                          *ADMIN SCHL SECUR MGR-MGL\n",
       "2         40003                                   *AGENCY ATTORNEY\n",
       "3         40004                             *ASSISTANT ADVOCATE-PD\n",
       "4         40005                       *ASSOCIATE EDUCATION OFFICER\n",
       "...         ...                                                ...\n",
       "1441      41442            DIRECTOR OF BUREAU OF CONSUMER SERVICES\n",
       "1442      41443  ASSOC ADM FOR PURCHASING MATERIALS MGT & ENVIR...\n",
       "1443      41444       PUBLIC HEALTH PREVENTATIVE MEDICINE RESIDENT\n",
       "1444      41445                          HOUSING ASSISTANT TRAINEE\n",
       "1445      41446                        SENIOR RACKETS INVESTIGATOR\n",
       "\n",
       "[1446 rows x 2 columns]>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TRANSFORM AND VALIDATE",
   "id": "8f6169933a427b2f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "from helpers.db_utils import read_table\n",
    "from helpers.alert_utils import send_urgent_email\n",
    "from helpers.metrics import rows_transformed, rows_validated, missing_values_detected\n",
    "\n",
    "def validate_and_clean_data(df, dim_col):\n",
    "    logging.info(f\"Validating and cleaning data\")\n",
    "\n",
    "    total_rows = len(df)\n",
    "    rows_validated.set(total_rows)  # Set the number of rows being validated\n",
    "\n",
    "    for col in dim_col:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "    # Check for missing values\n",
    "    missing = df.isnull().sum()\n",
    "    missing_values_detected.set(missing.sum())  # Set the total number of missing values detected\n",
    "\n",
    "    missing_percentage = (missing / total_rows) * 100\n",
    "\n",
    "    # Log changes\n",
    "    changes_log = []\n",
    "\n",
    "    # Handling based on missing value percentages\n",
    "    for col, pct in missing_percentage.items():\n",
    "        if pct <= 5:\n",
    "            df.dropna(subset=[col], inplace=True)\n",
    "            changes_log.append(f\"Dropped rows with missing values in {col} as it was <= 5%\")\n",
    "        elif 5 < pct <= 10:\n",
    "            if df[col].dtype == 'object':  # Replace with 'UNKNOWN' for strings\n",
    "                df[col].fillna('UNKNOWN', inplace=True)\n",
    "                changes_log.append(f\"Replaced missing string values in {col} with 'UNKNOWN'\")\n",
    "            else:\n",
    "                mean_value = df[col].mean()\n",
    "                df[col].fillna(mean_value, inplace=True)\n",
    "                changes_log.append(f\"Replaced missing numeric values in {col} with mean: {mean_value}\")\n",
    "        else:\n",
    "            logging.error(f\"Missing values in {col} exceed 10%. Manual intervention required.\")\n",
    "            send_urgent_email(\n",
    "                subject=f\"Data Quality Issue Detected in {col}\",\n",
    "                body=f\"High percentage of missing values in {col}: {pct}%. Immediate attention required.\",\n",
    "                to_email=\"data.engineer@example.com\"\n",
    "            )\n",
    "            raise ValueError(f\"High percentage of missing values in {col}: {pct}%\")\n",
    "\n",
    "    # Anomaly detection\n",
    "    for col in df.select_dtypes(include=['number']).columns:\n",
    "        # Replace negative values\n",
    "        if (df[col] < 0).any():\n",
    "            df.loc[df[col] < 0, col] = df[col].mean()\n",
    "            changes_log.append(f\"Replaced negative values in {col} with mean.\")\n",
    "\n",
    "        # Replace values greater than 2 * standard deviation\n",
    "        upper_bound = df[col].mean() + 2 * df[col].std()\n",
    "        if (df[col] > upper_bound).any():\n",
    "            df.loc[df[col] > upper_bound, col] = df[col].mean()\n",
    "            changes_log.append(f\"Replaced outliers in {col} (>{2} * SD) with mean.\")\n",
    "\n",
    "    # Log all changes made to a table or a file\n",
    "    logging.info(\"Data Cleaning Summary: \" + \"; \".join(changes_log))\n",
    "\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    rows_transformed.set(len(df))  # Set the number of rows transformed\n",
    "\n",
    "    return df\n",
    "\n",
    "def transform_master_data(df, required_columns):\n",
    "    return validate_and_clean_data(df, required_columns)\n",
    "\n",
    "def transform_transactional_data(df, engine):\n",
    "    df = validate_and_clean_data(df, ['EmployeeID', 'AgencyID', 'TitleCode'])\n",
    "\n",
    "    dim_employee = read_table(engine, 'DimEmployee')\n",
    "    df = pd.merge(df, dim_employee[['EmployeeID']], on='EmployeeID', how='left')\n",
    "\n",
    "    dim_agency = read_table(engine, 'DimAgency')\n",
    "    df = pd.merge(df, dim_agency[['AgencyID']], on='AgencyID', how='left')\n",
    "\n",
    "    dim_title = read_table(engine, 'DimTitle')\n",
    "    df = pd.merge(df, dim_title[['TitleCode']], on='TitleCode', how='left')\n",
    "\n",
    "    return df\n"
   ],
   "id": "5ebcbe9974e01aee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# LOADING AND INGESTION STAGE",
   "id": "7ef4fbb6663b305d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T14:12:29.495762Z",
     "start_time": "2024-08-25T14:12:28.746923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from helpers.db_utils import redshift_engine\n",
    "redshift_engine()"
   ],
   "id": "ab06eb1aa960b36c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-25 15:12:29,483 - root - INFO - Successfully created Redshift engine.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Engine(redshift+psycopg2://ridwanclouds:***@payroll-workgroup.637423632863.eu-west-2.redshift-serverless.amazonaws.com:5439/payrolldb)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T14:12:32.549113Z",
     "start_time": "2024-08-25T14:12:32.531288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from helpers.db_utils import redshift_engine, stage_data\n",
    "\n",
    "engine = redshift_engine()"
   ],
   "id": "c92c43f4198c1db7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-25 15:12:32,536 - root - INFO - Successfully created Redshift engine.\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T15:27:00.591383Z",
     "start_time": "2024-08-25T15:27:00.568049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "from helpers.metrics import rows_validated, missing_values_detected, rows_transformed, data_quality_issues\n",
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "def validate_and_clean_master_data(df, dim_columns):\n",
    "    logging.info(\"Validating and cleaning master data\")\n",
    "\n",
    "    # Record total rows before cleaning\n",
    "    total_rows = len(df)\n",
    "    rows_validated.set(total_rows)  # Set the number of rows being validated\n",
    "\n",
    "    # Ensure only columns in master_columns are present, drop any extra columns\n",
    "    available_columns = [col for col in dim_columns if col in df.columns]\n",
    "    df = df[available_columns]\n",
    "\n",
    "    # Initialize changes log\n",
    "    changes_log = []\n",
    "\n",
    "    # Validate and clean specific columns\n",
    "    for col in df.columns:\n",
    "        if col in ['EmployeeID', 'TitleCode', 'AgencyID']:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            changes_log.append(f\"Converted non-numeric values in {col} to NaN.\")\n",
    "        elif col in ['LastName', 'FirstName']:\n",
    "            df[col] = df[col].str.title()  # Standardize to title case\n",
    "            changes_log.append(f\"Standardized {col} to title case.\")\n",
    "\n",
    "    # Check for duplicates in key columns\n",
    "    key_columns = [col for col in ['EmployeeID', 'TitleCode', 'AgencyID'] if col in df.columns]\n",
    "    for col in key_columns:\n",
    "        if df[col].duplicated().any():\n",
    "            duplicated_values = df[col][df[col].duplicated()].tolist()\n",
    "            logging.error(f\"Duplicate values found in {col}: {duplicated_values}\")\n",
    "            data_quality_issues.inc()\n",
    "            df.drop_duplicates(subset=[col], keep='first', inplace=True)\n",
    "\n",
    "    # Check for missing values\n",
    "    missing = df.isnull().sum()\n",
    "    missing_values_detected.set(missing.sum())  # Set the total number of missing values detected\n",
    "\n",
    "    # Log all changes made to the master data\n",
    "    logging.info(\"Master Data Cleaning Summary: \" + \"; \".join(changes_log))\n",
    "\n",
    "    # Record the number of rows after cleaning\n",
    "    rows_transformed.set(len(df))  # Set the number of rows transformed\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "master_columns = ['EmployeeID', 'LastName', 'FirstName', 'TitleCode', 'TitleDescription', 'AgencyID', 'AgencyName']\n",
    "\n",
    "\n",
    "\n",
    "#df = extract_data('TitleMaster.csv')\n",
    "\n",
    "# Validate and clean the test DataFrame\n",
    "#cleaned_df = validate_and_clean_master_data(df, master_columns)\n",
    "#print(\"\\nCleaned DataFrame:\")\n",
    "#print(cleaned_df)"
   ],
   "id": "853472215051d8c7",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T15:11:42.705329Z",
     "start_time": "2024-08-25T15:11:42.648284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from helpers.alert_utils import send_urgent_email\n",
    "from helpers.metrics import rows_validated, missing_values_detected, rows_transformed, data_quality_issues\n",
    "\n",
    "def print_summary_report(df, initial_row_count, changes_log):\n",
    "    # Calculate current row count\n",
    "    current_row_count = len(df)\n",
    "\n",
    "    # Calculate number of duplicates\n",
    "    num_duplicates = initial_row_count - current_row_count\n",
    "    \n",
    "    # Calculate missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    total_missing_values = missing_values.sum()\n",
    "    \n",
    "    # Missing values by column\n",
    "    missing_values_report = missing_values[missing_values > 0]\n",
    "\n",
    "    # Count of cleaned data actions\n",
    "    cleaned_actions = [action for action in changes_log if 'Replaced' in action or 'Standardized' in action]\n",
    "\n",
    "    # Print summary report\n",
    "    print(\"Summary Report:\")\n",
    "    print(f\"Initial number of rows: {initial_row_count}\")\n",
    "    print(f\"Number of rows after cleaning: {current_row_count}\")\n",
    "    print(f\"Number of removed duplicates: {num_duplicates}\")\n",
    "    print(f\"Total missing values detected: {total_missing_values}\")\n",
    "\n",
    "    if not missing_values_report.empty:\n",
    "        print(\"\\nMissing values by column:\")\n",
    "        print(missing_values_report)\n",
    "    else:\n",
    "        print(\"No missing values detected.\")\n",
    "\n",
    "    if cleaned_actions:\n",
    "        print(\"\\nData Cleaning Actions:\")\n",
    "        for action in cleaned_actions:\n",
    "            print(f\" - {action}\")\n",
    "    else:\n",
    "        print(\"No specific cleaning actions were performed.\")\n",
    "\n",
    "\n",
    "def harmonize_columns(df):\n",
    "    # Map variations to a consistent column name\n",
    "    column_mapping = {\n",
    "        'AgencyCode': 'AgencyID',  # Harmonize AgencyCode to AgencyID\n",
    "    }\n",
    "    \n",
    "    # Rename columns according to the mapping\n",
    "    df.rename(columns=column_mapping, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def validate_and_clean_transactional_data(df, transaction_columns):\n",
    "    logging.info(\"Validating and cleaning transactional data\")\n",
    "    \n",
    "    df = harmonize_columns(df)\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    rows_validated.set(total_rows)  # Set the number of rows being validated\n",
    "\n",
    "    # Ensure all required columns are present and drop any extra columns\n",
    "    df = df[transaction_columns]\n",
    "\n",
    "    # Check for missing values\n",
    "    missing = df.isnull().sum()\n",
    "    missing_values_detected.set(missing.sum())  # Set the total number of missing values detected\n",
    "\n",
    "    missing_percentage = (missing / total_rows) * 100\n",
    "\n",
    "    changes_log = []\n",
    "    rows_before_cleaning = len(df)\n",
    "\n",
    "    # Handling missing values based on percentage\n",
    "    for col, pct in missing_percentage.items():\n",
    "        if pct <= 5:\n",
    "            df.dropna(subset=[col], inplace=True)\n",
    "            changes_log.append(f\"Dropped rows with missing values in {col} as it was <= 5%\")\n",
    "        elif 5 < pct <= 10:\n",
    "            if df[col].dtype == 'object':\n",
    "                df[col].fillna('UNKNOWN', inplace=True)\n",
    "                changes_log.append(f\"Replaced missing string values in {col} with 'UNKNOWN'\")\n",
    "            else:\n",
    "                mean_value = df[col].mean()\n",
    "                df[col].fillna(mean_value, inplace=True)\n",
    "                changes_log.append(f\"Replaced missing numeric values in {col} with mean: {mean_value}\")\n",
    "        else:\n",
    "            logging.error(f\"Missing values in {col} exceed 10%. Manual intervention required.\")\n",
    "            send_urgent_email(\n",
    "                subject=f\"Data Quality Issue Detected in {col}\",\n",
    "                body=f\"High percentage of missing values in {col}: {pct}%. Immediate attention required.\",\n",
    "                to_email=\"data.engineer@example.com\"\n",
    "            )\n",
    "            data_quality_issues.inc()\n",
    "            raise ValueError(f\"High percentage of missing values in {col}: {pct}%\")\n",
    "\n",
    "    # Handle anomalies for key columns separately\n",
    "    key_columns = ['EmployeeID', 'TitleCode', 'AgencyID', 'PayrollNumber']\n",
    "    for col in key_columns:\n",
    "        if col in df.columns:\n",
    "            # Convert non-numeric values to NaN for key columns\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            changes_log.append(f\"Converted non-numeric values in {col} to NaN.\")\n",
    "            # Fill missing values with NaN for key columns\n",
    "            df[col].fillna(pd.NA, inplace=True)\n",
    "\n",
    "    # Handle anomalies for measure columns\n",
    "    measure_columns = ['BaseSalary', 'RegularHours', 'RegularGrossPaid', 'OTHours', 'TotalOTPaid', 'TotalOtherPay']\n",
    "    for col in measure_columns:\n",
    "        if col in df.columns:\n",
    "            # Replace negative values with their positive equivalents\n",
    "            if (df[col] < 0).any():\n",
    "                df.loc[df[col] < 0, col] = df[col].abs()\n",
    "                changes_log.append(f\"Replaced negative values in {col} with their positive equivalents.\")\n",
    "            \n",
    "            # Handle outliers in measure columns\n",
    "            mean_value = df[col].mean()\n",
    "            std_dev = df[col].std()\n",
    "            upper_bound = mean_value + 2 * std_dev\n",
    "            lower_bound = mean_value - 2 * std_dev\n",
    "\n",
    "            outliers = (df[col] < lower_bound) | (df[col] > upper_bound)\n",
    "            if outliers.any():\n",
    "                df.loc[outliers, col] = mean_value  # Replace outliers with mean\n",
    "                changes_log.append(f\"Replaced outliers in {col} with mean value: {mean_value}\")\n",
    "\n",
    "    # Handle outliers in 'FiscalYear' column\n",
    "    if 'FiscalYear' in df.columns:\n",
    "        df['FiscalYear'] = pd.to_numeric(df['FiscalYear'], errors='coerce')\n",
    "        mean_value = df['FiscalYear'].mean()\n",
    "        std_dev = df['FiscalYear'].std()\n",
    "        upper_bound = mean_value + 2 * std_dev\n",
    "        lower_bound = mean_value - 2 * std_dev\n",
    "\n",
    "        outliers = (df['FiscalYear'] < lower_bound) | (df['FiscalYear'] > upper_bound)\n",
    "        if outliers.any():\n",
    "            most_frequent_year = df['FiscalYear'].mode()[0]  # Get the most frequent year\n",
    "            df.loc[outliers, 'FiscalYear'] = most_frequent_year\n",
    "            changes_log.append(f\"Replaced outlier FiscalYear values with most frequent year: {most_frequent_year}\")\n",
    "\n",
    "    # Standardize name columns\n",
    "    if 'FirstName' in df.columns:\n",
    "        df['FirstName'] = df['FirstName'].str.title()\n",
    "        changes_log.append(\"Standardized FirstName to title case.\")\n",
    "\n",
    "    if 'LastName' in df.columns:\n",
    "        df['LastName'] = df['LastName'].str.title()\n",
    "        changes_log.append(\"Standardized LastName to title case.\")\n",
    "\n",
    "    # Standardize categorical columns\n",
    "    categorical_columns = ['PayBasis', 'WorkLocationBorough']\n",
    "    for col in categorical_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].str.upper()\n",
    "            changes_log.append(f\"Standardized {col} to uppercase.\")\n",
    "\n",
    "    # Standardize date columns\n",
    "    if 'AgencyStartDate' in df.columns:\n",
    "        df['AgencyStartDate'] = pd.to_datetime(df['AgencyStartDate'], errors='coerce')\n",
    "        changes_log.append(\"Standardized AgencyStartDate to datetime format.\")\n",
    "\n",
    "    # Remove duplicate rows based on all columns\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    logging.info(\"Transactional Data Cleaning Summary: \" + \"; \".join(changes_log))\n",
    "\n",
    "    rows_transformed.set(len(df))  # Set the number of rows transformed\n",
    "\n",
    "    return df\n",
    "\n",
    "transaction_columns = [\n",
    "    'FiscalYear', 'PayrollNumber', 'AgencyID', 'AgencyName', 'EmployeeID', 'LastName', 'FirstName',\n",
    "    'AgencyStartDate', 'WorkLocationBorough', 'TitleCode', 'TitleDescription', 'LeaveStatusasofJune30',\n",
    "    'BaseSalary', 'PayBasis', 'RegularHours', 'RegularGrossPaid', 'OTHours', 'TotalOTPaid', 'TotalOtherPay'\n",
    "]\n",
    "\n",
    "#df = extract_data('nycpayroll_2021.csv')\n",
    "\n",
    "#initial_row_count = len(df)\n",
    "# changes_log = [] \n",
    "#c_df = validate_and_clean_transactional_data(df, transaction_columns)\n",
    "\n",
    "#print(c_df.head())\n",
    "#print_summary_report(c_df, initial_row_count, changes_log)"
   ],
   "id": "cf851327209f1446",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T15:11:45.746873Z",
     "start_time": "2024-08-25T15:11:45.738846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ensure_columns(df, columns):\n",
    "    \"\"\"Ensure the DataFrame has all the required columns, filling missing ones with NaN.\"\"\"\n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = pd.NA\n",
    "    return df[columns]\n"
   ],
   "id": "5cf4da255629e4b1",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T16:30:38.485185Z",
     "start_time": "2024-08-25T16:30:38.468349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "table_schemas = {\n",
    "    'dim_employee': ['EmployeeID', 'FirstName', 'LastName', 'LeaveStatusasofJune30'],\n",
    "    'dim_agency': ['AgencyID', 'AgencyName', 'AgencyStartDate'],\n",
    "    'dim_title': ['TitleCode', 'TitleDescription'],\n",
    "    'fact_payroll': ['PayrollNumber', 'EmployeeID', 'FiscalYear', 'BaseSalary', 'RegularHours',\n",
    "                     'RegularGrossPaid', 'OTHours', 'TotalOTPaid', 'TotalOtherPay', 'WorkLocationBorough']\n",
    "}\n",
    "\n",
    "dim_columns = [\n",
    "    ['EmployeeID', 'LastName', 'FirstName', 'LeaveStatusasofJune30'],  # Columns for dim_employee\n",
    "    ['TitleCode', 'TitleDescription'],  # Columns for dim_title\n",
    "    ['AgencyID', 'AgencyName', 'AgencyStartDate']  # Columns for dim_agency\n",
    "]\n",
    "\n",
    "master_files = ['EmpMaster.csv', 'TitleMaster.csv', 'AgencyMaster.csv']\n",
    "\n",
    "dim_table_names = ['dim_employee', 'dim_title', 'dim_agency']\n"
   ],
   "id": "264b22d6c34f7504",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T15:20:51.127196Z",
     "start_time": "2024-08-25T15:18:24.131767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from helpers.db_utils import stage_data\n",
    "\n",
    "def transform_master_data(master_files):\n",
    "    # Initialize empty DataFrames with the required columns\n",
    "    dim_employee_df = pd.DataFrame(columns=table_schemas['dim_employee'])\n",
    "    dim_agency_df = pd.DataFrame(columns=table_schemas['dim_agency'])\n",
    "    dim_title_df = pd.DataFrame(columns=table_schemas['dim_title'])\n",
    "\n",
    "    # Define a mapping from file names to dimension table names and their schemas\n",
    "    file_to_table_map = {\n",
    "        'EmpMaster.csv': ('dim_employee', table_schemas['dim_employee']),\n",
    "        'AgencyMaster.csv': ('dim_agency', table_schemas['dim_agency']),\n",
    "        'TitleMaster.csv': ('dim_title', table_schemas['dim_title'])\n",
    "    }\n",
    "\n",
    "    for file_name in master_files:\n",
    "        table_name, required_columns = file_to_table_map.get(file_name)\n",
    "\n",
    "        if table_name:\n",
    "            # Extract data from the file\n",
    "            df = extract_data(file_name)\n",
    "\n",
    "            # Validate and clean the master data\n",
    "            df_cleaned = validate_and_clean_master_data(df, required_columns)\n",
    "\n",
    "            # Ensure the DataFrame has all required columns\n",
    "            df_cleaned = ensure_columns(df_cleaned, required_columns)\n",
    "\n",
    "            # Append data to the appropriate dimension DataFrame\n",
    "            if table_name == 'dim_employee':\n",
    "                dim_employee_df = pd.concat([\n",
    "                    dim_employee_df,\n",
    "                    df_cleaned.drop_duplicates()\n",
    "                ], ignore_index=True)\n",
    "\n",
    "            elif table_name == 'dim_agency':\n",
    "                dim_agency_df = pd.concat([\n",
    "                    dim_agency_df,\n",
    "                    df_cleaned.drop_duplicates()\n",
    "                ], ignore_index=True)\n",
    "\n",
    "            elif table_name == 'dim_title':\n",
    "                dim_title_df = pd.concat([\n",
    "                    dim_title_df,\n",
    "                    df_cleaned.drop_duplicates()\n",
    "                ], ignore_index=True)\n",
    "    \n",
    "    total_master_rows = len(dim_employee_df) + len(dim_agency_df) + len(dim_title_df)\n",
    "    \n",
    "    stage_data(engine, dim_employee_df, 'dim_employee')\n",
    "    stage_data(engine, dim_agency_df, 'dim_agency')\n",
    "    stage_data(engine, dim_title_df, 'dim_title')\n",
    "    print(f\"Master data successfully transformed and staged.\")\n",
    "    print(f\" - dim_employee: {len(dim_employee_df)} rows\")\n",
    "    print(f\" - dim_agency: {len(dim_agency_df)} rows\")\n",
    "    print(f\" - dim_title: {len(dim_title_df)} rows\")\n",
    "    print(f\"Total master data staged: {len(dim_employee_df) + len(dim_agency_df) + len(dim_title_df)} rows\")\n",
    "\n",
    "transform_master_data(master_files)"
   ],
   "id": "967a254906173c0e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-25 16:18:24,156 - root - INFO - Extracting EmpMaster.csv from S3\n",
      "2024-08-25 16:18:24,322 - root - INFO - Validating and cleaning master data\n",
      "2024-08-25 16:18:24,338 - root - INFO - Master Data Cleaning Summary: Cleaned leading non-alphabetic characters from all string columns.; Converted non-numeric values in EmployeeID to NaN.; Standardized FirstName to title case.; Standardized LastName to title case.\n",
      "2024-08-25 16:18:24,358 - root - INFO - Extracting TitleMaster.csv from S3\n",
      "2024-08-25 16:18:24,440 - root - INFO - Validating and cleaning master data\n",
      "2024-08-25 16:18:24,449 - root - INFO - Master Data Cleaning Summary: Cleaned leading non-alphabetic characters from all string columns.; Converted non-numeric values in TitleCode to NaN.\n",
      "2024-08-25 16:18:24,459 - root - INFO - Extracting AgencyMaster.csv from S3\n",
      "2024-08-25 16:18:24,529 - root - INFO - Validating and cleaning master data\n",
      "2024-08-25 16:18:24,536 - root - INFO - Master Data Cleaning Summary: Cleaned leading non-alphabetic characters from all string columns.; Converted non-numeric values in AgencyID to NaN.\n",
      "2024-08-25 16:19:23,832 - root - INFO - Data successfully staged to staging_dim_employee.\n",
      "2024-08-25 16:19:32,711 - root - INFO - Data successfully staged to staging_dim_agency.\n",
      "2024-08-25 16:20:51,121 - root - INFO - Data successfully staged to staging_dim_title.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master data successfully transformed and staged.\n",
      " - dim_employee: 1000 rows\n",
      " - dim_agency: 153 rows\n",
      " - dim_title: 1446 rows\n",
      "Total master data staged: 2599 rows\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T16:07:49.436959Z",
     "start_time": "2024-08-22T16:07:49.432746Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f911d56f035b1c2b",
   "outputs": [],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T16:33:13.240267Z",
     "start_time": "2024-08-25T16:32:20.322133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "def transform_transactional_data(payroll_files):\n",
    "    # Initialize empty DataFrames for dimensions\n",
    "    dim_employee_df = pd.DataFrame(columns=table_schemas['dim_employee'])\n",
    "    dim_agency_df = pd.DataFrame(columns=table_schemas['dim_agency'])\n",
    "    dim_title_df = pd.DataFrame(columns=table_schemas['dim_title'])\n",
    "    \n",
    "    fact_payroll_df = pd.DataFrame(columns=table_schemas['fact_payroll'])\n",
    "\n",
    "    for file_name in payroll_files:\n",
    "        # Extract data from the transactional files\n",
    "        df = extract_from_s3(s3_client, s3_bucket, s3_prefix, file_name)\n",
    "\n",
    "        # Clean and validate the transactional data\n",
    "        df_cleaned = validate_and_clean_transactional_data(df, transaction_columns)\n",
    "\n",
    "        # Update dimension DataFrames\n",
    "        # For dim_employee\n",
    "        dim_employee_data = df_cleaned[['EmployeeID', 'FirstName', 'LastName', 'LeaveStatusasofJune30']]\n",
    "        dim_employee_df = pd.concat([\n",
    "            dim_employee_df,\n",
    "            ensure_columns(dim_employee_data, table_schemas['dim_employee']).drop_duplicates()\n",
    "        ], ignore_index=True)\n",
    "        \n",
    "        # For dim_agency\n",
    "        dim_agency_data = df_cleaned[['AgencyID', 'AgencyName', 'AgencyStartDate']]\n",
    "        dim_agency_df = pd.concat([\n",
    "            dim_agency_df,\n",
    "            ensure_columns(dim_agency_data, table_schemas['dim_agency']).drop_duplicates()\n",
    "        ], ignore_index=True)\n",
    "        \n",
    "        # For dim_title\n",
    "        dim_title_data = df_cleaned[['TitleCode', 'TitleDescription']]\n",
    "        dim_title_df = pd.concat([\n",
    "            dim_title_df,\n",
    "            ensure_columns(dim_title_data, table_schemas['dim_title']).drop_duplicates()\n",
    "        ], ignore_index=True)\n",
    "        \n",
    "        # Prepare fact_payroll DataFrame\n",
    "        fact_payroll_data = df_cleaned[['PayrollNumber', 'EmployeeID', 'FiscalYear', 'BaseSalary',\n",
    "                                        'RegularHours', 'RegularGrossPaid', 'OTHours', 'TotalOTPaid', 'TotalOtherPay',\n",
    "                                        'WorkLocationBorough']]\n",
    "        fact_payroll_df = pd.concat([\n",
    "            fact_payroll_df,\n",
    "            ensure_columns(fact_payroll_data, table_schemas['fact_payroll']).drop_duplicates()\n",
    "        ], ignore_index=True)\n",
    "\n",
    "    \n",
    "    total_transactional_rows = len(fact_payroll_df)\n",
    "    \n",
    "    stage_data(engine, dim_employee_df, 'dim_employee')\n",
    "    stage_data(engine, dim_agency_df, 'dim_agency')\n",
    "    stage_data(engine, dim_title_df, 'dim_title')\n",
    "    stage_data(engine, fact_payroll_df, 'fact_payroll')\n",
    "    \n",
    "    print(f\"Transactional data successfully transformed and staged.\")\n",
    "    print(f\" - fact_payroll: {len(fact_payroll_df)} rows\")\n",
    "    print(f\"Total transactional data staged: {len(fact_payroll_df)} rows\")\n",
    "    total_rows = total_master_rows + total_transactional_row\n",
    "    print(f\"All data successfully transformed and staged.\")\n",
    "    print(f\" - Total master data: {total_master_rows} rows\")\n",
    "    print(f\" - Total transactional data: {total_transactional_rows} rows\")\n",
    "    print(f\"Total data staged: {total_rows} rows\")\n",
    "transform_transactional_data(payroll_files)"
   ],
   "id": "89ca08b88f16b9f0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-25 17:32:20,359 - root - INFO - Extracting nycpayroll_2020.csv from S3\n",
      "2024-08-25 17:32:20,565 - root - INFO - Validating and cleaning transactional data\n",
      "2024-08-25 17:32:20,754 - root - INFO - Transactional Data Cleaning Summary: Dropped rows with missing values in FiscalYear as it was <= 5%; Dropped rows with missing values in PayrollNumber as it was <= 5%; Dropped rows with missing values in AgencyID as it was <= 5%; Dropped rows with missing values in AgencyName as it was <= 5%; Dropped rows with missing values in EmployeeID as it was <= 5%; Dropped rows with missing values in LastName as it was <= 5%; Dropped rows with missing values in FirstName as it was <= 5%; Dropped rows with missing values in AgencyStartDate as it was <= 5%; Dropped rows with missing values in WorkLocationBorough as it was <= 5%; Dropped rows with missing values in TitleCode as it was <= 5%; Dropped rows with missing values in TitleDescription as it was <= 5%; Dropped rows with missing values in LeaveStatusasofJune30 as it was <= 5%; Dropped rows with missing values in BaseSalary as it was <= 5%; Dropped rows with missing values in PayBasis as it was <= 5%; Dropped rows with missing values in RegularHours as it was <= 5%; Dropped rows with missing values in RegularGrossPaid as it was <= 5%; Dropped rows with missing values in OTHours as it was <= 5%; Dropped rows with missing values in TotalOTPaid as it was <= 5%; Dropped rows with missing values in TotalOtherPay as it was <= 5%; Converted non-numeric values in EmployeeID to NaN.; Converted non-numeric values in TitleCode to NaN.; Converted non-numeric values in AgencyID to NaN.; Converted non-numeric values in PayrollNumber to NaN.; Replaced outliers in BaseSalary with mean value: 94344.41990000001; Replaced outliers in RegularGrossPaid with mean value: 61417.81569999999; Replaced outliers in OTHours with mean value: 72.78; Replaced outliers in TotalOTPaid with mean value: 3355.0415000000003; Replaced negative values in TotalOtherPay with their positive equivalents.; Replaced outliers in TotalOtherPay with mean value: 5406.189; Replaced outlier FiscalYear values with most frequent year: 2020; Standardized FirstName to title case.; Standardized LastName to title case.; Standardized PayBasis to uppercase.; Standardized WorkLocationBorough to uppercase.; Standardized AgencyStartDate to datetime format.\n",
      "2024-08-25 17:32:20,795 - root - INFO - Extracting nycpayroll_2021.csv from S3\n",
      "2024-08-25 17:32:20,869 - root - INFO - Validating and cleaning transactional data\n",
      "2024-08-25 17:32:20,980 - root - INFO - Transactional Data Cleaning Summary: Dropped rows with missing values in FiscalYear as it was <= 5%; Dropped rows with missing values in PayrollNumber as it was <= 5%; Dropped rows with missing values in AgencyID as it was <= 5%; Dropped rows with missing values in AgencyName as it was <= 5%; Dropped rows with missing values in EmployeeID as it was <= 5%; Dropped rows with missing values in LastName as it was <= 5%; Dropped rows with missing values in FirstName as it was <= 5%; Dropped rows with missing values in AgencyStartDate as it was <= 5%; Dropped rows with missing values in WorkLocationBorough as it was <= 5%; Dropped rows with missing values in TitleCode as it was <= 5%; Dropped rows with missing values in TitleDescription as it was <= 5%; Dropped rows with missing values in LeaveStatusasofJune30 as it was <= 5%; Dropped rows with missing values in BaseSalary as it was <= 5%; Dropped rows with missing values in PayBasis as it was <= 5%; Dropped rows with missing values in RegularHours as it was <= 5%; Dropped rows with missing values in RegularGrossPaid as it was <= 5%; Dropped rows with missing values in OTHours as it was <= 5%; Dropped rows with missing values in TotalOTPaid as it was <= 5%; Dropped rows with missing values in TotalOtherPay as it was <= 5%; Converted non-numeric values in EmployeeID to NaN.; Converted non-numeric values in TitleCode to NaN.; Converted non-numeric values in AgencyID to NaN.; Converted non-numeric values in PayrollNumber to NaN.; Replaced outliers in BaseSalary with mean value: 108447.15782178221; Replaced outliers in RegularHours with mean value: 1941.4059405940593; Replaced outliers in RegularGrossPaid with mean value: 165771.6408910891; Replaced negative values in OTHours with their positive equivalents.; Replaced outliers in OTHours with mean value: 1017.1178217821783; Replaced negative values in TotalOTPaid with their positive equivalents.; Replaced negative values in TotalOtherPay with their positive equivalents.; Replaced outliers in TotalOtherPay with mean value: 30303.672277227717; Replaced outlier FiscalYear values with most frequent year: 2021; Standardized FirstName to title case.; Standardized LastName to title case.; Standardized PayBasis to uppercase.; Standardized WorkLocationBorough to uppercase.; Standardized AgencyStartDate to datetime format.\n",
      "2024-08-25 17:32:32,780 - root - INFO - Data successfully staged to staging_dim_employee.\n",
      "2024-08-25 17:32:43,537 - root - INFO - Data successfully staged to staging_dim_agency.\n",
      "2024-08-25 17:32:46,399 - root - INFO - Data successfully staged to staging_dim_title.\n",
      "2024-08-25 17:33:13,109 - root - INFO - Data successfully staged to staging_fact_payroll.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transactional data successfully transformed and staged.\n",
      " - fact_payroll: 201 rows\n",
      "Total transactional data staged: 201 rows\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'total_master_rows' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[46], line 65\u001B[0m\n\u001B[1;32m     63\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m - Total transactional data: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtotal_transactional_rows\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m rows\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     64\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTotal data staged: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtotal_rows\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m rows\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 65\u001B[0m \u001B[43mtransform_transactional_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpayroll_files\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[46], line 60\u001B[0m, in \u001B[0;36mtransform_transactional_data\u001B[0;34m(payroll_files)\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m - fact_payroll: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(fact_payroll_df)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m rows\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTotal transactional data staged: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(fact_payroll_df)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m rows\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 60\u001B[0m total_rows \u001B[38;5;241m=\u001B[39m \u001B[43mtotal_master_rows\u001B[49m \u001B[38;5;241m+\u001B[39m total_transactional_row\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAll data successfully transformed and staged.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m - Total master data: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtotal_master_rows\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m rows\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'total_master_rows' is not defined"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T16:43:36.211410Z",
     "start_time": "2024-08-22T16:43:36.208021Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "278825a2c01ea3c2",
   "outputs": [],
   "execution_count": 122
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ffa5a720f4a4f484"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "94fef79a7091d6db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "89f316fdf18775f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "893636250cb8c7cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1789b1e77272490e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "658804db9eda27d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "725ff1871586dfd9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "148a8663dc0cfb73"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T11:19:34.748221Z",
     "start_time": "2024-08-20T11:19:34.734353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from helpers.db_utils import redshift_engine, stage_data, create_fact_table\n",
    "from helpers.metrics import rows_staged,fact_table_created\n",
    "from sqlalchemy import MetaData, Table, Column, Integer, String\n",
    "import logging\n",
    "\n",
    "metadata = MetaData()\n",
    "engine = redshift_engine()\n",
    "\n",
    "def load_master_data(df, table_name, engine):\n",
    "    logging.info(f\"Loading master data into {table_name}\")\n",
    "    stage_data(engine, df, table_name)\n",
    "    rows_staged.set(len(df))\n",
    "\n",
    "def load_transactional_data(df, engine):\n",
    "    logging.info(f\"Loading transactional data into FactPayroll\")\n",
    "    create_fact_table(engine, engine.metadata)\n",
    "    fact_table_created.set(1)\n",
    "    stage_data(engine, df, 'FactPayroll')\n",
    "    rows_staged.set(len(df))\n"
   ],
   "id": "a67c5202c036d3ed",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "def ingest_master_data():\n",
    "\n",
    "\n",
    "    for file_name, table_name, dim_col in zip(master_files, master_table_names, dim_columns):\n",
    "        df = extract.extract_from_s3(s3_bucket, s3_prefix, file_name)\n",
    "        df_transformed = transform.transform_master_data(df, dim_col)\n",
    "        stage_data(df_transformed, table_name)\n",
    "\n",
    "\n",
    "def ingest_transactional_data():\n",
    "    logging.info(f\"Ingesting transactional data\")\n",
    "    s3_bucket = 'your-s3-bucket-name'\n",
    "    s3_prefix = 'your-folder-prefix/'\n",
    "    payroll_files = ['nycpayroll_2020.csv', 'nycpayroll_2021.csv']\n",
    "\n",
    "    create_fact_table()\n",
    "\n",
    "    for file_name in payroll_files:\n",
    "        df = extract.extract_from_s3(s3_bucket, s3_prefix, file_name)\n",
    "        df_transformed = transform.transform_transactional_data(df, engine)\n",
    "        stage_data(df_transformed, 'FactPayroll')"
   ],
   "id": "bb4885ce4271e343"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
