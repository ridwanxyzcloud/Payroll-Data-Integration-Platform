{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-20T11:15:19.524007Z",
     "start_time": "2024-08-20T11:15:19.020770Z"
    }
   },
   "source": "import pandas as pd",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T11:15:24.315911Z",
     "start_time": "2024-08-20T11:15:22.113592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "\n",
    "import os\n",
    "from io import StringIO\n",
    "\n",
    "from helpers.metrics import start_metrics_server, files_extracted, rows_extracted, rows_transformed, rows_validated, missing_values_detected, rows_staged, fact_table_created, rows_processed, rows_cleaned, data_quality_issues \n",
    "from helpers.logging_utils import setup_logging\n",
    "\n",
    "setup_logging()\n",
    "load_dotenv(override=True)\n",
    "\n",
    "start_metrics_server(port=8001)"
   ],
   "id": "d1798b6ef16a075c",
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 48] Address already in use",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 14\u001B[0m\n\u001B[1;32m     11\u001B[0m setup_logging()\n\u001B[1;32m     12\u001B[0m load_dotenv(override\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m---> 14\u001B[0m \u001B[43mstart_metrics_server\u001B[49m\u001B[43m(\u001B[49m\u001B[43mport\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m8001\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/Payroll-Data-Integration-Platform/helpers/metrics.py:18\u001B[0m, in \u001B[0;36mstart_metrics_server\u001B[0;34m(port)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstart_metrics_server\u001B[39m(port\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8000\u001B[39m):\n\u001B[0;32m---> 18\u001B[0m     \u001B[43mstart_http_server\u001B[49m\u001B[43m(\u001B[49m\u001B[43mport\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/Payroll-Data-Integration-Platform/.venv/lib/python3.9/site-packages/prometheus_client/exposition.py:221\u001B[0m, in \u001B[0;36mstart_wsgi_server\u001B[0;34m(port, addr, registry, certfile, keyfile, client_cafile, client_capath, protocol, client_auth_required)\u001B[0m\n\u001B[1;32m    219\u001B[0m TmpServer\u001B[38;5;241m.\u001B[39maddress_family, addr \u001B[38;5;241m=\u001B[39m _get_best_family(addr, port)\n\u001B[1;32m    220\u001B[0m app \u001B[38;5;241m=\u001B[39m make_wsgi_app(registry)\n\u001B[0;32m--> 221\u001B[0m httpd \u001B[38;5;241m=\u001B[39m \u001B[43mmake_server\u001B[49m\u001B[43m(\u001B[49m\u001B[43maddr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mport\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mapp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mTmpServer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhandler_class\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_SilentHandler\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    222\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m certfile \u001B[38;5;129;01mand\u001B[39;00m keyfile:\n\u001B[1;32m    223\u001B[0m     context \u001B[38;5;241m=\u001B[39m _get_ssl_ctx(certfile, keyfile, protocol, client_cafile, client_capath, client_auth_required)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/wsgiref/simple_server.py:154\u001B[0m, in \u001B[0;36mmake_server\u001B[0;34m(host, port, app, server_class, handler_class)\u001B[0m\n\u001B[1;32m    150\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmake_server\u001B[39m(\n\u001B[1;32m    151\u001B[0m     host, port, app, server_class\u001B[38;5;241m=\u001B[39mWSGIServer, handler_class\u001B[38;5;241m=\u001B[39mWSGIRequestHandler\n\u001B[1;32m    152\u001B[0m ):\n\u001B[1;32m    153\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Create a new WSGI server listening on `host` and `port` for `app`\"\"\"\u001B[39;00m\n\u001B[0;32m--> 154\u001B[0m     server \u001B[38;5;241m=\u001B[39m \u001B[43mserver_class\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhost\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mport\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhandler_class\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    155\u001B[0m     server\u001B[38;5;241m.\u001B[39mset_app(app)\n\u001B[1;32m    156\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m server\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/socketserver.py:452\u001B[0m, in \u001B[0;36mTCPServer.__init__\u001B[0;34m(self, server_address, RequestHandlerClass, bind_and_activate)\u001B[0m\n\u001B[1;32m    450\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m bind_and_activate:\n\u001B[1;32m    451\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 452\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mserver_bind\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    453\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mserver_activate()\n\u001B[1;32m    454\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m:\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/wsgiref/simple_server.py:50\u001B[0m, in \u001B[0;36mWSGIServer.server_bind\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mserver_bind\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m     49\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Override server_bind to store the server name.\"\"\"\u001B[39;00m\n\u001B[0;32m---> 50\u001B[0m     \u001B[43mHTTPServer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mserver_bind\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msetup_environ()\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/server.py:136\u001B[0m, in \u001B[0;36mHTTPServer.server_bind\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mserver_bind\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    135\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Override server_bind to store the server name.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 136\u001B[0m     \u001B[43msocketserver\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTCPServer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mserver_bind\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    137\u001B[0m     host, port \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mserver_address[:\u001B[38;5;241m2\u001B[39m]\n\u001B[1;32m    138\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mserver_name \u001B[38;5;241m=\u001B[39m socket\u001B[38;5;241m.\u001B[39mgetfqdn(host)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/socketserver.py:466\u001B[0m, in \u001B[0;36mTCPServer.server_bind\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    464\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mallow_reuse_address:\n\u001B[1;32m    465\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket\u001B[38;5;241m.\u001B[39msetsockopt(socket\u001B[38;5;241m.\u001B[39mSOL_SOCKET, socket\u001B[38;5;241m.\u001B[39mSO_REUSEADDR, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m--> 466\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msocket\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbind\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mserver_address\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    467\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mserver_address \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket\u001B[38;5;241m.\u001B[39mgetsockname()\n",
      "\u001B[0;31mOSError\u001B[0m: [Errno 48] Address already in use"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# EXTRACTION STAGE",
   "id": "dc58ae88a2a142ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T11:15:30.696811Z",
     "start_time": "2024-08-20T11:15:30.243140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# File definitions\n",
    "master_files = ['EmpMaster.csv', 'TitleMaster.csv', 'AgencyMaster.csv']\n",
    "payroll_files = ['nycpayroll_2020.csv', 'nycpayroll_2021.csv']\n",
    "master_table_names = ['DimEmployee', 'DimTitle', 'DimAgency']\n",
    "dim_columns = [\n",
    "    ['EmployeeID', 'LastName', 'FirstName', 'LeaveStatusasofJune30'],\n",
    "    ['TitleCode', 'TitleDescription'],\n",
    "    ['AgencyID', 'AgencyName', 'AgencyStartDate']\n",
    "]\n",
    "\n",
    "# AWS and S3 configuration\n",
    "s3_bucket = os.getenv(\"s3_bucket\")\n",
    "s3_prefix = os.getenv(\"s3_prefix\")\n",
    "aws_region = os.getenv(\"aws_region\")\n",
    "aws_access_key_id = os.getenv(\"aws_access_key_id\")\n",
    "aws_secret_access_key = os.getenv(\"aws_secret_access_key\")\n",
    "\n",
    "def initialize_s3_client(aws_region, aws_access_key_id, aws_secret_access_key):\n",
    "    return boto3.client(\n",
    "        's3',\n",
    "        region_name=aws_region,\n",
    "        aws_access_key_id=aws_access_key_id,\n",
    "        aws_secret_access_key=aws_secret_access_key\n",
    "    )\n",
    "\n",
    "s3_client = initialize_s3_client(aws_region, aws_access_key_id, aws_secret_access_key)\n",
    "\n",
    "\n",
    "def extract_from_s3(s3_client, s3_bucket, s3_prefix, file_name):\n",
    "    try:\n",
    "        logging.info(f\"Extracting {file_name} from S3\")\n",
    "        obj = s3_client.get_object(Bucket=s3_bucket, Key=s3_prefix + file_name)\n",
    "        df = pd.read_csv(StringIO(obj['Body'].read().decode('utf-8')))\n",
    "\n",
    "        # Update metrics\n",
    "        files_extracted.inc()  # Increment the count of files extracted\n",
    "\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to extract {file_name}: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "def extract_data(file_name):\n",
    "    return extract_from_s3(s3_client, s3_bucket, s3_prefix, file_name)\n",
    "\n"
   ],
   "id": "64a0ccb6ba642622",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T11:15:33.141541Z",
     "start_time": "2024-08-20T11:15:32.933864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_test = extract_data('EmpMaster.csv')\n",
    "df_test.head(3)"
   ],
   "id": "e11dfeb516df9b04",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-20 12:15:32,938 - root - INFO - Extracting EmpMaster.csv from S3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   EmployeeID LastName FirstName\n",
       "0      100001   AACHEN     DAVID\n",
       "1      100002   AACHEN    MONICA\n",
       "2      100003   AADAMS   LAMMELL"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EmployeeID</th>\n",
       "      <th>LastName</th>\n",
       "      <th>FirstName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>AACHEN</td>\n",
       "      <td>DAVID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100002</td>\n",
       "      <td>AACHEN</td>\n",
       "      <td>MONICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100003</td>\n",
       "      <td>AADAMS</td>\n",
       "      <td>LAMMELL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TRANSFORM AND VALIDATE",
   "id": "8f6169933a427b2f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T11:15:41.436373Z",
     "start_time": "2024-08-20T11:15:41.039888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from helpers.db_utils import read_table\n",
    "from helpers.alert_utils import send_urgent_email\n",
    "from helpers.metrics import rows_transformed, rows_validated, missing_values_detected\n",
    "\n",
    "def validate_and_clean_data(df, dim_col):\n",
    "    logging.info(f\"Validating and cleaning data\")\n",
    "\n",
    "    total_rows = len(df)\n",
    "    rows_validated.set(total_rows)  # Set the number of rows being validated\n",
    "\n",
    "    for col in dim_col:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "    # Check for missing values\n",
    "    missing = df.isnull().sum()\n",
    "    missing_values_detected.set(missing.sum())  # Set the total number of missing values detected\n",
    "\n",
    "    missing_percentage = (missing / total_rows) * 100\n",
    "\n",
    "    # Log changes\n",
    "    changes_log = []\n",
    "\n",
    "    # Handling based on missing value percentages\n",
    "    for col, pct in missing_percentage.items():\n",
    "        if pct <= 5:\n",
    "            df.dropna(subset=[col], inplace=True)\n",
    "            changes_log.append(f\"Dropped rows with missing values in {col} as it was <= 5%\")\n",
    "        elif 5 < pct <= 10:\n",
    "            if df[col].dtype == 'object':  # Replace with 'UNKNOWN' for strings\n",
    "                df[col].fillna('UNKNOWN', inplace=True)\n",
    "                changes_log.append(f\"Replaced missing string values in {col} with 'UNKNOWN'\")\n",
    "            else:\n",
    "                mean_value = df[col].mean()\n",
    "                df[col].fillna(mean_value, inplace=True)\n",
    "                changes_log.append(f\"Replaced missing numeric values in {col} with mean: {mean_value}\")\n",
    "        else:\n",
    "            logging.error(f\"Missing values in {col} exceed 10%. Manual intervention required.\")\n",
    "            send_urgent_email(\n",
    "                subject=f\"Data Quality Issue Detected in {col}\",\n",
    "                body=f\"High percentage of missing values in {col}: {pct}%. Immediate attention required.\",\n",
    "                to_email=\"data.engineer@example.com\"\n",
    "            )\n",
    "            raise ValueError(f\"High percentage of missing values in {col}: {pct}%\")\n",
    "\n",
    "    # Anomaly detection\n",
    "    for col in df.select_dtypes(include=['number']).columns:\n",
    "        # Replace negative values\n",
    "        if (df[col] < 0).any():\n",
    "            df.loc[df[col] < 0, col] = df[col].mean()\n",
    "            changes_log.append(f\"Replaced negative values in {col} with mean.\")\n",
    "\n",
    "        # Replace values greater than 2 * standard deviation\n",
    "        upper_bound = df[col].mean() + 2 * df[col].std()\n",
    "        if (df[col] > upper_bound).any():\n",
    "            df.loc[df[col] > upper_bound, col] = df[col].mean()\n",
    "            changes_log.append(f\"Replaced outliers in {col} (>{2} * SD) with mean.\")\n",
    "\n",
    "    # Log all changes made to a table or a file\n",
    "    logging.info(\"Data Cleaning Summary: \" + \"; \".join(changes_log))\n",
    "\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    rows_transformed.set(len(df))  # Set the number of rows transformed\n",
    "\n",
    "    return df\n",
    "\n",
    "def transform_master_data(df, required_columns):\n",
    "    return validate_and_clean_data(df, required_columns)\n",
    "\n",
    "def transform_transactional_data(df, engine):\n",
    "    df = validate_and_clean_data(df, ['EmployeeID', 'AgencyID', 'TitleCode'])\n",
    "\n",
    "    dim_employee = read_table(engine, 'DimEmployee')\n",
    "    df = pd.merge(df, dim_employee[['EmployeeID']], on='EmployeeID', how='left')\n",
    "\n",
    "    dim_agency = read_table(engine, 'DimAgency')\n",
    "    df = pd.merge(df, dim_agency[['AgencyID']], on='AgencyID', how='left')\n",
    "\n",
    "    dim_title = read_table(engine, 'DimTitle')\n",
    "    df = pd.merge(df, dim_title[['TitleCode']], on='TitleCode', how='left')\n",
    "\n",
    "    return df\n"
   ],
   "id": "9b80394e8f795a07",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# LOADING AND INGESTION STAGE",
   "id": "7ef4fbb6663b305d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T11:23:30.509021Z",
     "start_time": "2024-08-20T11:23:30.477308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from helpers.db_utils import redshift_engine\n",
    "redshift_engine()"
   ],
   "id": "ab06eb1aa960b36c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-20 12:23:30,482 - root - INFO - Successfully created Redshift engine.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Engine(redshift+psycopg2://ridwanclouds:***@payroll-workgroup.637423632863.eu-west-2.redshift-serverless.amazonaws.com:5439/payrolldb)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T12:03:50.487547Z",
     "start_time": "2024-08-20T12:03:50.409606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from helpers.db_utils import redshift_engine, stage_data\n",
    "\n",
    "engine = redshift_engine()\n",
    "df = df_test\n",
    "table_name = 'test'\n",
    "stage_data(engine, df, table_name)\n"
   ],
   "id": "c92c43f4198c1db7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-20 13:03:50,413 - root - INFO - Successfully created Redshift engine.\n",
      "2024-08-20 13:03:50,482 - root - ERROR - Failed to stage data to staging_test: (psycopg2.OperationalError) could not translate host name \"payroll-workgroup.637423632863.eu-west-2.redshift-serverless.amazonaws.com\" to address: nodename nor servname provided, or not known\n",
      "\n",
      "(Background on this error at: https://sqlalche.me/e/14/e3q8)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T12:13:00.552400Z",
     "start_time": "2024-08-20T12:13:00.491442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from helpers.redshift_utils import redshift_engine2, stage_data\n",
    "\n",
    "engine = redshift_engine2()\n",
    "df = df_test\n",
    "table_name = 'test'\n",
    "stage_data(engine, df, table_name)\n"
   ],
   "id": "d6fbf19024a9b0db",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-20 13:13:00,494 - root - INFO - Successfully created Redshift engine.\n",
      "2024-08-20 13:13:00,548 - root - ERROR - Failed to stage data to staging_test: (psycopg2.OperationalError) could not translate host name \"payroll-workgroup.637423632863.eu-west-2.redshift-serverless.amazonaws.com\" to address: nodename nor servname provided, or not known\n",
      "\n",
      "(Background on this error at: https://sqlalche.me/e/14/e3q8)\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "853472215051d8c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T11:19:34.748221Z",
     "start_time": "2024-08-20T11:19:34.734353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from helpers.db_utils import redshift_engine, stage_data, create_fact_table\n",
    "from helpers.metrics import rows_staged,fact_table_created\n",
    "from sqlalchemy import MetaData, Table, Column, Integer, String\n",
    "import logging\n",
    "\n",
    "metadata = MetaData()\n",
    "\n",
    "\n",
    "def load_master_data(df, table_name, engine):\n",
    "    logging.info(f\"Loading master data into {table_name}\")\n",
    "    stage_data(engine, df, table_name)\n",
    "    rows_staged.set(len(df))\n",
    "\n",
    "def load_transactional_data(df, engine):\n",
    "    logging.info(f\"Loading transactional data into FactPayroll\")\n",
    "    create_fact_table(engine, engine.metadata)\n",
    "    fact_table_created.set(1)\n",
    "    stage_data(engine, df, 'FactPayroll')\n",
    "    rows_staged.set(len(df))\n"
   ],
   "id": "a67c5202c036d3ed",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bb4885ce4271e343"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
