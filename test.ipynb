{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-20T19:59:48.994571Z",
     "start_time": "2024-08-20T19:59:47.914141Z"
    }
   },
   "source": "import pandas as pd",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T15:01:14.672559Z",
     "start_time": "2024-08-22T15:01:14.490431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "\n",
    "import os\n",
    "from io import StringIO\n",
    "\n",
    "from helpers.metrics import start_metrics_server, files_extracted, rows_extracted, rows_transformed, rows_validated, missing_values_detected, rows_staged, fact_table_created, rows_processed, rows_cleaned, data_quality_issues \n",
    "from helpers.logging_utils import setup_logging\n",
    "\n",
    "setup_logging()\n",
    "load_dotenv(override=True)\n",
    "\n",
    "start_metrics_server(port=8001)"
   ],
   "id": "d1798b6ef16a075c",
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 48] Address already in use",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[92], line 14\u001B[0m\n\u001B[1;32m     11\u001B[0m setup_logging()\n\u001B[1;32m     12\u001B[0m load_dotenv(override\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m---> 14\u001B[0m \u001B[43mstart_metrics_server\u001B[49m\u001B[43m(\u001B[49m\u001B[43mport\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m8001\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/Payroll-Data-Integration-Platform/helpers/metrics.py:18\u001B[0m, in \u001B[0;36mstart_metrics_server\u001B[0;34m(port)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstart_metrics_server\u001B[39m(port\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8000\u001B[39m):\n\u001B[0;32m---> 18\u001B[0m     \u001B[43mstart_http_server\u001B[49m\u001B[43m(\u001B[49m\u001B[43mport\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/Payroll-Data-Integration-Platform/.venv/lib/python3.9/site-packages/prometheus_client/exposition.py:221\u001B[0m, in \u001B[0;36mstart_wsgi_server\u001B[0;34m(port, addr, registry, certfile, keyfile, client_cafile, client_capath, protocol, client_auth_required)\u001B[0m\n\u001B[1;32m    219\u001B[0m TmpServer\u001B[38;5;241m.\u001B[39maddress_family, addr \u001B[38;5;241m=\u001B[39m _get_best_family(addr, port)\n\u001B[1;32m    220\u001B[0m app \u001B[38;5;241m=\u001B[39m make_wsgi_app(registry)\n\u001B[0;32m--> 221\u001B[0m httpd \u001B[38;5;241m=\u001B[39m \u001B[43mmake_server\u001B[49m\u001B[43m(\u001B[49m\u001B[43maddr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mport\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mapp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mTmpServer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhandler_class\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_SilentHandler\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    222\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m certfile \u001B[38;5;129;01mand\u001B[39;00m keyfile:\n\u001B[1;32m    223\u001B[0m     context \u001B[38;5;241m=\u001B[39m _get_ssl_ctx(certfile, keyfile, protocol, client_cafile, client_capath, client_auth_required)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/wsgiref/simple_server.py:154\u001B[0m, in \u001B[0;36mmake_server\u001B[0;34m(host, port, app, server_class, handler_class)\u001B[0m\n\u001B[1;32m    150\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmake_server\u001B[39m(\n\u001B[1;32m    151\u001B[0m     host, port, app, server_class\u001B[38;5;241m=\u001B[39mWSGIServer, handler_class\u001B[38;5;241m=\u001B[39mWSGIRequestHandler\n\u001B[1;32m    152\u001B[0m ):\n\u001B[1;32m    153\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Create a new WSGI server listening on `host` and `port` for `app`\"\"\"\u001B[39;00m\n\u001B[0;32m--> 154\u001B[0m     server \u001B[38;5;241m=\u001B[39m \u001B[43mserver_class\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhost\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mport\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhandler_class\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    155\u001B[0m     server\u001B[38;5;241m.\u001B[39mset_app(app)\n\u001B[1;32m    156\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m server\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/socketserver.py:452\u001B[0m, in \u001B[0;36mTCPServer.__init__\u001B[0;34m(self, server_address, RequestHandlerClass, bind_and_activate)\u001B[0m\n\u001B[1;32m    450\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m bind_and_activate:\n\u001B[1;32m    451\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 452\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mserver_bind\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    453\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mserver_activate()\n\u001B[1;32m    454\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m:\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/wsgiref/simple_server.py:50\u001B[0m, in \u001B[0;36mWSGIServer.server_bind\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mserver_bind\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m     49\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Override server_bind to store the server name.\"\"\"\u001B[39;00m\n\u001B[0;32m---> 50\u001B[0m     \u001B[43mHTTPServer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mserver_bind\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msetup_environ()\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/http/server.py:136\u001B[0m, in \u001B[0;36mHTTPServer.server_bind\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mserver_bind\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    135\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Override server_bind to store the server name.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 136\u001B[0m     \u001B[43msocketserver\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTCPServer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mserver_bind\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    137\u001B[0m     host, port \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mserver_address[:\u001B[38;5;241m2\u001B[39m]\n\u001B[1;32m    138\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mserver_name \u001B[38;5;241m=\u001B[39m socket\u001B[38;5;241m.\u001B[39mgetfqdn(host)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/socketserver.py:466\u001B[0m, in \u001B[0;36mTCPServer.server_bind\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    464\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mallow_reuse_address:\n\u001B[1;32m    465\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket\u001B[38;5;241m.\u001B[39msetsockopt(socket\u001B[38;5;241m.\u001B[39mSOL_SOCKET, socket\u001B[38;5;241m.\u001B[39mSO_REUSEADDR, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m--> 466\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msocket\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbind\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mserver_address\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    467\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mserver_address \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket\u001B[38;5;241m.\u001B[39mgetsockname()\n",
      "\u001B[0;31mOSError\u001B[0m: [Errno 48] Address already in use"
     ]
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# EXTRACTION STAGE",
   "id": "dc58ae88a2a142ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T15:01:23.014025Z",
     "start_time": "2024-08-22T15:01:22.961870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# File definitions\n",
    "master_files = ['EmpMaster.csv', 'TitleMaster.csv', 'AgencyMaster.csv']\n",
    "payroll_files = ['nycpayroll_2020.csv', 'nycpayroll_2021.csv']\n",
    "master_table_names = ['DimEmployee', 'DimTitle', 'DimAgency']\n",
    "dim_columns = [\n",
    "    ['EmployeeID', 'LastName', 'FirstName', 'LeaveStatusasofJune30'],\n",
    "    ['TitleCode', 'TitleDescription'],\n",
    "    ['AgencyID', 'AgencyName', 'AgencyStartDate']\n",
    "]\n",
    "\n",
    "# AWS and S3 configuration\n",
    "s3_bucket = os.getenv(\"s3_bucket\")\n",
    "s3_prefix = os.getenv(\"s3_prefix\")\n",
    "aws_region = os.getenv(\"aws_region\")\n",
    "aws_access_key_id = os.getenv(\"aws_access_key_id\")\n",
    "aws_secret_access_key = os.getenv(\"aws_secret_access_key\")\n",
    "\n",
    "def initialize_s3_client(aws_region, aws_access_key_id, aws_secret_access_key):\n",
    "    return boto3.client(\n",
    "        's3',\n",
    "        region_name=aws_region,\n",
    "        aws_access_key_id=aws_access_key_id,\n",
    "        aws_secret_access_key=aws_secret_access_key\n",
    "    )\n",
    "\n",
    "s3_client = initialize_s3_client(aws_region, aws_access_key_id, aws_secret_access_key)\n",
    "\n",
    "\n",
    "def extract_from_s3(s3_client, s3_bucket, s3_prefix, file_name):\n",
    "    try:\n",
    "        logging.info(f\"Extracting {file_name} from S3\")\n",
    "        obj = s3_client.get_object(Bucket=s3_bucket, Key=s3_prefix + file_name)\n",
    "        df = pd.read_csv(StringIO(obj['Body'].read().decode('utf-8')))\n",
    "\n",
    "        # Update metrics\n",
    "        files_extracted.inc()  # Increment the count of files extracted\n",
    "\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to extract {file_name}: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "def extract_data(file_name):\n",
    "    return extract_from_s3(s3_client, s3_bucket, s3_prefix, file_name)\n",
    "\n"
   ],
   "id": "64a0ccb6ba642622",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T15:01:25.961125Z",
     "start_time": "2024-08-22T15:01:25.675854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_test = extract_data('EmpMaster.csv')\n",
    "df_test.head()"
   ],
   "id": "e11dfeb516df9b04",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-22 16:01:25,677 - root - INFO - Extracting EmpMaster.csv from S3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   EmployeeID LastName FirstName\n",
       "0      100001   AACHEN     DAVID\n",
       "1      100002   AACHEN    MONICA\n",
       "2      100003   AADAMS   LAMMELL\n",
       "3      100004    AADIL      IRIS\n",
       "4      100005   AALAAM      AMIR"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EmployeeID</th>\n",
       "      <th>LastName</th>\n",
       "      <th>FirstName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>AACHEN</td>\n",
       "      <td>DAVID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100002</td>\n",
       "      <td>AACHEN</td>\n",
       "      <td>MONICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100003</td>\n",
       "      <td>AADAMS</td>\n",
       "      <td>LAMMELL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100004</td>\n",
       "      <td>AADIL</td>\n",
       "      <td>IRIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100005</td>\n",
       "      <td>AALAAM</td>\n",
       "      <td>AMIR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TRANSFORM AND VALIDATE",
   "id": "8f6169933a427b2f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "from helpers.db_utils import read_table\n",
    "from helpers.alert_utils import send_urgent_email\n",
    "from helpers.metrics import rows_transformed, rows_validated, missing_values_detected\n",
    "\n",
    "def validate_and_clean_data(df, dim_col):\n",
    "    logging.info(f\"Validating and cleaning data\")\n",
    "\n",
    "    total_rows = len(df)\n",
    "    rows_validated.set(total_rows)  # Set the number of rows being validated\n",
    "\n",
    "    for col in dim_col:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "    # Check for missing values\n",
    "    missing = df.isnull().sum()\n",
    "    missing_values_detected.set(missing.sum())  # Set the total number of missing values detected\n",
    "\n",
    "    missing_percentage = (missing / total_rows) * 100\n",
    "\n",
    "    # Log changes\n",
    "    changes_log = []\n",
    "\n",
    "    # Handling based on missing value percentages\n",
    "    for col, pct in missing_percentage.items():\n",
    "        if pct <= 5:\n",
    "            df.dropna(subset=[col], inplace=True)\n",
    "            changes_log.append(f\"Dropped rows with missing values in {col} as it was <= 5%\")\n",
    "        elif 5 < pct <= 10:\n",
    "            if df[col].dtype == 'object':  # Replace with 'UNKNOWN' for strings\n",
    "                df[col].fillna('UNKNOWN', inplace=True)\n",
    "                changes_log.append(f\"Replaced missing string values in {col} with 'UNKNOWN'\")\n",
    "            else:\n",
    "                mean_value = df[col].mean()\n",
    "                df[col].fillna(mean_value, inplace=True)\n",
    "                changes_log.append(f\"Replaced missing numeric values in {col} with mean: {mean_value}\")\n",
    "        else:\n",
    "            logging.error(f\"Missing values in {col} exceed 10%. Manual intervention required.\")\n",
    "            send_urgent_email(\n",
    "                subject=f\"Data Quality Issue Detected in {col}\",\n",
    "                body=f\"High percentage of missing values in {col}: {pct}%. Immediate attention required.\",\n",
    "                to_email=\"data.engineer@example.com\"\n",
    "            )\n",
    "            raise ValueError(f\"High percentage of missing values in {col}: {pct}%\")\n",
    "\n",
    "    # Anomaly detection\n",
    "    for col in df.select_dtypes(include=['number']).columns:\n",
    "        # Replace negative values\n",
    "        if (df[col] < 0).any():\n",
    "            df.loc[df[col] < 0, col] = df[col].mean()\n",
    "            changes_log.append(f\"Replaced negative values in {col} with mean.\")\n",
    "\n",
    "        # Replace values greater than 2 * standard deviation\n",
    "        upper_bound = df[col].mean() + 2 * df[col].std()\n",
    "        if (df[col] > upper_bound).any():\n",
    "            df.loc[df[col] > upper_bound, col] = df[col].mean()\n",
    "            changes_log.append(f\"Replaced outliers in {col} (>{2} * SD) with mean.\")\n",
    "\n",
    "    # Log all changes made to a table or a file\n",
    "    logging.info(\"Data Cleaning Summary: \" + \"; \".join(changes_log))\n",
    "\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    rows_transformed.set(len(df))  # Set the number of rows transformed\n",
    "\n",
    "    return df\n",
    "\n",
    "def transform_master_data(df, required_columns):\n",
    "    return validate_and_clean_data(df, required_columns)\n",
    "\n",
    "def transform_transactional_data(df, engine):\n",
    "    df = validate_and_clean_data(df, ['EmployeeID', 'AgencyID', 'TitleCode'])\n",
    "\n",
    "    dim_employee = read_table(engine, 'DimEmployee')\n",
    "    df = pd.merge(df, dim_employee[['EmployeeID']], on='EmployeeID', how='left')\n",
    "\n",
    "    dim_agency = read_table(engine, 'DimAgency')\n",
    "    df = pd.merge(df, dim_agency[['AgencyID']], on='AgencyID', how='left')\n",
    "\n",
    "    dim_title = read_table(engine, 'DimTitle')\n",
    "    df = pd.merge(df, dim_title[['TitleCode']], on='TitleCode', how='left')\n",
    "\n",
    "    return df\n"
   ],
   "id": "5ebcbe9974e01aee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# LOADING AND INGESTION STAGE",
   "id": "7ef4fbb6663b305d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T15:01:36.053285Z",
     "start_time": "2024-08-22T15:01:36.033164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from helpers.db_utils import redshift_engine\n",
    "redshift_engine()"
   ],
   "id": "ab06eb1aa960b36c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-22 16:01:36,036 - root - INFO - Successfully created Redshift engine.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Engine(redshift+psycopg2://ridwanclouds:***@payroll-workgroup.637423632863.eu-west-2.redshift-serverless.amazonaws.com:5439/payrolldb)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T15:01:45.588083Z",
     "start_time": "2024-08-22T15:01:45.568675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from helpers.db_utils import redshift_engine, stage_data\n",
    "\n",
    "engine = redshift_engine()"
   ],
   "id": "c92c43f4198c1db7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-22 16:01:45,573 - root - INFO - Successfully created Redshift engine.\n"
     ]
    }
   ],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T15:08:52.883573Z",
     "start_time": "2024-08-22T15:08:52.866120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "from helpers.metrics import rows_validated, missing_values_detected, rows_transformed, data_quality_issues\n",
    "\n",
    "def validate_and_clean_master_data(df, dim_columns):\n",
    "    logging.info(\"Validating and cleaning master data\")\n",
    "\n",
    "    # Record total rows before cleaning\n",
    "    total_rows = len(df)\n",
    "    rows_validated.set(total_rows)  # Set the number of rows being validated\n",
    "\n",
    "    # Ensure only columns in master_columns are present, drop any extra columns\n",
    "    available_columns = [col for col in dim_columns if col in df.columns]\n",
    "    df = df[available_columns]\n",
    "\n",
    "    # Initialize changes log\n",
    "    changes_log = []\n",
    "\n",
    "    # Validate and clean specific columns\n",
    "    for col in df.columns:\n",
    "        if col in ['EmployeeID', 'TitleCode', 'AgencyID']:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            changes_log.append(f\"Converted non-numeric values in {col} to NaN.\")\n",
    "        elif col in ['LastName', 'FirstName']:\n",
    "            df[col] = df[col].str.title()  # Standardize to title case\n",
    "            changes_log.append(f\"Standardized {col} to title case.\")\n",
    "\n",
    "    # Check for duplicates in key columns\n",
    "    key_columns = [col for col in ['EmployeeID', 'TitleCode', 'AgencyID'] if col in df.columns]\n",
    "    for col in key_columns:\n",
    "        if df[col].duplicated().any():\n",
    "            duplicated_values = df[col][df[col].duplicated()].tolist()\n",
    "            logging.error(f\"Duplicate values found in {col}: {duplicated_values}\")\n",
    "            data_quality_issues.inc()\n",
    "            df.drop_duplicates(subset=[col], keep='first', inplace=True)\n",
    "\n",
    "    # Check for missing values\n",
    "    missing = df.isnull().sum()\n",
    "    missing_values_detected.set(missing.sum())  # Set the total number of missing values detected\n",
    "\n",
    "    # Log all changes made to the master data\n",
    "    logging.info(\"Master Data Cleaning Summary: \" + \"; \".join(changes_log))\n",
    "\n",
    "    # Record the number of rows after cleaning\n",
    "    rows_transformed.set(len(df))  # Set the number of rows transformed\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "master_columns = ['EmployeeID', 'LastName', 'FirstName', 'TitleCode', 'TitleDescription', 'AgencyID', 'AgencyName']\n",
    "\n",
    "\n",
    "\n",
    "#df = extract_data('TitleMaster.csv')\n",
    "\n",
    "# Validate and clean the test DataFrame\n",
    "#cleaned_df = validate_and_clean_master_data(df, master_columns)\n",
    "#print(\"\\nCleaned DataFrame:\")\n",
    "#print(cleaned_df)"
   ],
   "id": "853472215051d8c7",
   "outputs": [],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T15:09:23.869114Z",
     "start_time": "2024-08-22T15:09:23.823035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from helpers.alert_utils import send_urgent_email\n",
    "from helpers.metrics import rows_validated, missing_values_detected, rows_transformed, data_quality_issues\n",
    "\n",
    "def print_summary_report(df, initial_row_count, changes_log):\n",
    "    # Calculate current row count\n",
    "    current_row_count = len(df)\n",
    "\n",
    "    # Calculate number of duplicates\n",
    "    num_duplicates = initial_row_count - current_row_count\n",
    "    \n",
    "    # Calculate missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    total_missing_values = missing_values.sum()\n",
    "    \n",
    "    # Missing values by column\n",
    "    missing_values_report = missing_values[missing_values > 0]\n",
    "\n",
    "    # Count of cleaned data actions\n",
    "    cleaned_actions = [action for action in changes_log if 'Replaced' in action or 'Standardized' in action]\n",
    "\n",
    "    # Print summary report\n",
    "    print(\"Summary Report:\")\n",
    "    print(f\"Initial number of rows: {initial_row_count}\")\n",
    "    print(f\"Number of rows after cleaning: {current_row_count}\")\n",
    "    print(f\"Number of removed duplicates: {num_duplicates}\")\n",
    "    print(f\"Total missing values detected: {total_missing_values}\")\n",
    "\n",
    "    if not missing_values_report.empty:\n",
    "        print(\"\\nMissing values by column:\")\n",
    "        print(missing_values_report)\n",
    "    else:\n",
    "        print(\"No missing values detected.\")\n",
    "\n",
    "    if cleaned_actions:\n",
    "        print(\"\\nData Cleaning Actions:\")\n",
    "        for action in cleaned_actions:\n",
    "            print(f\" - {action}\")\n",
    "    else:\n",
    "        print(\"No specific cleaning actions were performed.\")\n",
    "\n",
    "\n",
    "def harmonize_columns(df):\n",
    "    # Map variations to a consistent column name\n",
    "    column_mapping = {\n",
    "        'AgencyCode': 'AgencyID',  # Harmonize AgencyCode to AgencyID\n",
    "    }\n",
    "    \n",
    "    # Rename columns according to the mapping\n",
    "    df.rename(columns=column_mapping, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def validate_and_clean_transactional_data(df, transaction_columns):\n",
    "    logging.info(\"Validating and cleaning transactional data\")\n",
    "    \n",
    "    df = harmonize_columns(df)\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    rows_validated.set(total_rows)  # Set the number of rows being validated\n",
    "\n",
    "    # Ensure all required columns are present and drop any extra columns\n",
    "    df = df[transaction_columns]\n",
    "\n",
    "    # Check for missing values\n",
    "    missing = df.isnull().sum()\n",
    "    missing_values_detected.set(missing.sum())  # Set the total number of missing values detected\n",
    "\n",
    "    missing_percentage = (missing / total_rows) * 100\n",
    "\n",
    "    changes_log = []\n",
    "    rows_before_cleaning = len(df)\n",
    "\n",
    "    # Handling missing values based on percentage\n",
    "    for col, pct in missing_percentage.items():\n",
    "        if pct <= 5:\n",
    "            df.dropna(subset=[col], inplace=True)\n",
    "            changes_log.append(f\"Dropped rows with missing values in {col} as it was <= 5%\")\n",
    "        elif 5 < pct <= 10:\n",
    "            if df[col].dtype == 'object':\n",
    "                df[col].fillna('UNKNOWN', inplace=True)\n",
    "                changes_log.append(f\"Replaced missing string values in {col} with 'UNKNOWN'\")\n",
    "            else:\n",
    "                mean_value = df[col].mean()\n",
    "                df[col].fillna(mean_value, inplace=True)\n",
    "                changes_log.append(f\"Replaced missing numeric values in {col} with mean: {mean_value}\")\n",
    "        else:\n",
    "            logging.error(f\"Missing values in {col} exceed 10%. Manual intervention required.\")\n",
    "            send_urgent_email(\n",
    "                subject=f\"Data Quality Issue Detected in {col}\",\n",
    "                body=f\"High percentage of missing values in {col}: {pct}%. Immediate attention required.\",\n",
    "                to_email=\"data.engineer@example.com\"\n",
    "            )\n",
    "            data_quality_issues.inc()\n",
    "            raise ValueError(f\"High percentage of missing values in {col}: {pct}%\")\n",
    "\n",
    "    # Handle anomalies for key columns separately\n",
    "    key_columns = ['EmployeeID', 'TitleCode', 'AgencyID', 'PayrollNumber']\n",
    "    for col in key_columns:\n",
    "        if col in df.columns:\n",
    "            # Convert non-numeric values to NaN for key columns\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            changes_log.append(f\"Converted non-numeric values in {col} to NaN.\")\n",
    "            # Fill missing values with NaN for key columns\n",
    "            df[col].fillna(pd.NA, inplace=True)\n",
    "\n",
    "    # Handle anomalies for measure columns\n",
    "    measure_columns = ['BaseSalary', 'RegularHours', 'RegularGrossPaid', 'OTHours', 'TotalOTPaid', 'TotalOtherPay']\n",
    "    for col in measure_columns:\n",
    "        if col in df.columns:\n",
    "            # Replace negative values with their positive equivalents\n",
    "            if (df[col] < 0).any():\n",
    "                df.loc[df[col] < 0, col] = df[col].abs()\n",
    "                changes_log.append(f\"Replaced negative values in {col} with their positive equivalents.\")\n",
    "            \n",
    "            # Handle outliers in measure columns\n",
    "            mean_value = df[col].mean()\n",
    "            std_dev = df[col].std()\n",
    "            upper_bound = mean_value + 2 * std_dev\n",
    "            lower_bound = mean_value - 2 * std_dev\n",
    "\n",
    "            outliers = (df[col] < lower_bound) | (df[col] > upper_bound)\n",
    "            if outliers.any():\n",
    "                df.loc[outliers, col] = mean_value  # Replace outliers with mean\n",
    "                changes_log.append(f\"Replaced outliers in {col} with mean value: {mean_value}\")\n",
    "\n",
    "    # Handle outliers in 'FiscalYear' column\n",
    "    if 'FiscalYear' in df.columns:\n",
    "        df['FiscalYear'] = pd.to_numeric(df['FiscalYear'], errors='coerce')\n",
    "        mean_value = df['FiscalYear'].mean()\n",
    "        std_dev = df['FiscalYear'].std()\n",
    "        upper_bound = mean_value + 2 * std_dev\n",
    "        lower_bound = mean_value - 2 * std_dev\n",
    "\n",
    "        outliers = (df['FiscalYear'] < lower_bound) | (df['FiscalYear'] > upper_bound)\n",
    "        if outliers.any():\n",
    "            most_frequent_year = df['FiscalYear'].mode()[0]  # Get the most frequent year\n",
    "            df.loc[outliers, 'FiscalYear'] = most_frequent_year\n",
    "            changes_log.append(f\"Replaced outlier FiscalYear values with most frequent year: {most_frequent_year}\")\n",
    "\n",
    "    # Standardize name columns\n",
    "    if 'FirstName' in df.columns:\n",
    "        df['FirstName'] = df['FirstName'].str.title()\n",
    "        changes_log.append(\"Standardized FirstName to title case.\")\n",
    "\n",
    "    if 'LastName' in df.columns:\n",
    "        df['LastName'] = df['LastName'].str.title()\n",
    "        changes_log.append(\"Standardized LastName to title case.\")\n",
    "\n",
    "    # Standardize categorical columns\n",
    "    categorical_columns = ['PayBasis', 'WorkLocationBorough']\n",
    "    for col in categorical_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].str.upper()\n",
    "            changes_log.append(f\"Standardized {col} to uppercase.\")\n",
    "\n",
    "    # Standardize date columns\n",
    "    if 'AgencyStartDate' in df.columns:\n",
    "        df['AgencyStartDate'] = pd.to_datetime(df['AgencyStartDate'], errors='coerce')\n",
    "        changes_log.append(\"Standardized AgencyStartDate to datetime format.\")\n",
    "\n",
    "    # Remove duplicate rows based on all columns\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    logging.info(\"Transactional Data Cleaning Summary: \" + \"; \".join(changes_log))\n",
    "\n",
    "    rows_transformed.set(len(df))  # Set the number of rows transformed\n",
    "\n",
    "    return df\n",
    "\n",
    "transaction_columns = [\n",
    "    'FiscalYear', 'PayrollNumber', 'AgencyID', 'AgencyName', 'EmployeeID', 'LastName', 'FirstName',\n",
    "    'AgencyStartDate', 'WorkLocationBorough', 'TitleCode', 'TitleDescription', 'LeaveStatusasofJune30',\n",
    "    'BaseSalary', 'PayBasis', 'RegularHours', 'RegularGrossPaid', 'OTHours', 'TotalOTPaid', 'TotalOtherPay'\n",
    "]\n",
    "\n",
    "#df = extract_data('nycpayroll_2021.csv')\n",
    "\n",
    "#initial_row_count = len(df)\n",
    "# changes_log = [] \n",
    "#c_df = validate_and_clean_transactional_data(df, transaction_columns)\n",
    "\n",
    "#print(c_df.head())\n",
    "#print_summary_report(c_df, initial_row_count, changes_log)"
   ],
   "id": "cf851327209f1446",
   "outputs": [],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T16:10:03.396534Z",
     "start_time": "2024-08-22T16:10:03.388286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ensure_columns(df, columns):\n",
    "    \"\"\"Ensure the DataFrame has all the required columns, filling missing ones with NaN.\"\"\"\n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = pd.NA\n",
    "    return df[columns]\n"
   ],
   "id": "5cf4da255629e4b1",
   "outputs": [],
   "execution_count": 114
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T16:36:07.996713Z",
     "start_time": "2024-08-22T16:36:07.978868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "table_schemas = {\n",
    "    'dim_employee': ['EmployeeID', 'FirstName', 'LastName', 'LeaveStatusasofJune30'],\n",
    "    'dim_agency': ['AgencyID', 'AgencyName', 'AgencyStartDate'],\n",
    "    'dim_title': ['TitleCode', 'TitleDescription'],\n",
    "    'fact_payroll': ['PayrollID', 'EmployeeID', 'FiscalYear', 'PayrollNumber', 'BaseSalary', 'RegularHours',\n",
    "                     'RegularGrossPaid', 'OTHours', 'TotalOTPaid', 'TotalOtherPay', 'WorkLocationBorough']\n",
    "}\n",
    "\n",
    "dim_columns = [\n",
    "    ['EmployeeID', 'LastName', 'FirstName', 'LeaveStatusasofJune30'],  # Columns for dim_employee\n",
    "    ['TitleCode', 'TitleDescription'],  # Columns for dim_title\n",
    "    ['AgencyID', 'AgencyName', 'AgencyStartDate']  # Columns for dim_agency\n",
    "]\n",
    "\n",
    "master_files = ['EmpMaster.csv', 'TitleMaster.csv', 'AgencyMaster.csv']\n",
    "\n",
    "dim_table_names = ['dim_employee', 'dim_title', 'dim_agency']\n"
   ],
   "id": "264b22d6c34f7504",
   "outputs": [],
   "execution_count": 121
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T17:12:25.253765Z",
     "start_time": "2024-08-22T17:12:24.836816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from helpers.db_utils import stage_data\n",
    "\n",
    "\n",
    "def transform_master_data(master_files):\n",
    "    # Initialize empty DataFrames with the required columns\n",
    "    dim_employee_df = pd.DataFrame(columns=table_schemas['dim_employee'])\n",
    "    dim_agency_df = pd.DataFrame(columns=table_schemas['dim_agency'])\n",
    "    dim_title_df = pd.DataFrame(columns=table_schemas['dim_title'])\n",
    "\n",
    "    # Define a mapping from file names to dimension table names and their schemas\n",
    "    file_to_table_map = {\n",
    "        'EmpMaster.csv': ('dim_employee', table_schemas['dim_employee']),\n",
    "        'AgencyMaster.csv': ('dim_agency', table_schemas['dim_agency']),\n",
    "        'TitleMaster.csv': ('dim_title', table_schemas['dim_title'])\n",
    "    }\n",
    "\n",
    "    for file_name in master_files:\n",
    "        table_name, required_columns = file_to_table_map.get(file_name)\n",
    "\n",
    "        if table_name:\n",
    "            # Extract data from the file\n",
    "            df = extract_data(file_name)\n",
    "\n",
    "            # Validate and clean the master data\n",
    "            df_cleaned = validate_and_clean_master_data(df, required_columns)\n",
    "\n",
    "            # Ensure the DataFrame has all required columns\n",
    "            df_cleaned = ensure_columns(df_cleaned, required_columns)\n",
    "\n",
    "            # Append data to the appropriate dimension DataFrame\n",
    "            if table_name == 'dim_employee':\n",
    "                dim_employee_df = pd.concat([\n",
    "                    dim_employee_df,\n",
    "                    df_cleaned.drop_duplicates()\n",
    "                ], ignore_index=True)\n",
    "\n",
    "            elif table_name == 'dim_agency':\n",
    "                dim_agency_df = pd.concat([\n",
    "                    dim_agency_df,\n",
    "                    df_cleaned.drop_duplicates()\n",
    "                ], ignore_index=True)\n",
    "\n",
    "            elif table_name == 'dim_title':\n",
    "                dim_title_df = pd.concat([\n",
    "                    dim_title_df,\n",
    "                    df_cleaned.drop_duplicates()\n",
    "                ], ignore_index=True)\n",
    "\n",
    "    return dim_employee_df, dim_agency_df, dim_title_df\n",
    "    \n",
    "transform_master_data(master_files)"
   ],
   "id": "967a254906173c0e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-22 18:12:24,857 - root - INFO - Extracting EmpMaster.csv from S3\n",
      "2024-08-22 18:12:25,051 - root - INFO - Validating and cleaning master data\n",
      "2024-08-22 18:12:25,064 - root - INFO - Master Data Cleaning Summary: Converted non-numeric values in EmployeeID to NaN.; Standardized FirstName to title case.; Standardized LastName to title case.\n",
      "2024-08-22 18:12:25,075 - root - INFO - Extracting TitleMaster.csv from S3\n",
      "2024-08-22 18:12:25,144 - root - INFO - Validating and cleaning master data\n",
      "2024-08-22 18:12:25,151 - root - INFO - Master Data Cleaning Summary: Converted non-numeric values in TitleCode to NaN.\n",
      "2024-08-22 18:12:25,160 - root - INFO - Extracting AgencyMaster.csv from S3\n",
      "2024-08-22 18:12:25,216 - root - INFO - Validating and cleaning master data\n",
      "2024-08-22 18:12:25,220 - root - INFO - Master Data Cleaning Summary: Converted non-numeric values in AgencyID to NaN.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(    EmployeeID FirstName LastName LeaveStatusasofJune30\n",
       " 0       100001     David   Aachen                  <NA>\n",
       " 1       100002    Monica   Aachen                  <NA>\n",
       " 2       100003   Lammell   Aadams                  <NA>\n",
       " 3       100004      Iris    Aadil                  <NA>\n",
       " 4       100005      Amir   Aalaam                  <NA>\n",
       " ..         ...       ...      ...                   ...\n",
       " 995     100996     Ramon    Abreu                  <NA>\n",
       " 996     100997    Ramona    Abreu                  <NA>\n",
       " 997     100998   Raymond    Abreu                  <NA>\n",
       " 998     100999      Rigo    Abreu                  <NA>\n",
       " 999     101000      Rita    Abreu                  <NA>\n",
       " \n",
       " [1000 rows x 4 columns],\n",
       "     AgencyID                      AgencyName AgencyStartDate\n",
       " 0       2001       ADMIN FOR CHILDREN'S SVCS            <NA>\n",
       " 1       2002       ADMIN TRIALS AND HEARINGS            <NA>\n",
       " 2       2003             BOARD OF CORRECTION            <NA>\n",
       " 3       2004               BOARD OF ELECTION            <NA>\n",
       " 4       2005  BOARD OF ELECTION POLL WORKERS            <NA>\n",
       " ..       ...                             ...             ...\n",
       " 148     2149   STATEN ISLAND COMMUNITY BD #1            <NA>\n",
       " 149     2150   STATEN ISLAND COMMUNITY BD #2            <NA>\n",
       " 150     2151                  TAX COMMISSION            <NA>\n",
       " 151     2152     TAXI & LIMOUSINE COMMISSION            <NA>\n",
       " 152     2153      TEACHERS RETIREMENT SYSTEM            <NA>\n",
       " \n",
       " [153 rows x 3 columns],\n",
       "      TitleCode                                   TitleDescription\n",
       " 0        40001                     *ADM SCHOOL SECURITY MANAGER-U\n",
       " 1        40002                          *ADMIN SCHL SECUR MGR-MGL\n",
       " 2        40003                                   *AGENCY ATTORNEY\n",
       " 3        40004                             *ASSISTANT ADVOCATE-PD\n",
       " 4        40005                       *ASSOCIATE EDUCATION OFFICER\n",
       " ...        ...                                                ...\n",
       " 1441     41442            DIRECTOR OF BUREAU OF CONSUMER SERVICES\n",
       " 1442     41443  ASSOC ADM FOR PURCHASING MATERIALS MGT & ENVIR...\n",
       " 1443     41444       PUBLIC HEALTH PREVENTATIVE MEDICINE RESIDENT\n",
       " 1444     41445                          HOUSING ASSISTANT TRAINEE\n",
       " 1445     41446                        SENIOR RACKETS INVESTIGATOR\n",
       " \n",
       " [1446 rows x 2 columns])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 123
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T16:07:49.436959Z",
     "start_time": "2024-08-22T16:07:49.432746Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f911d56f035b1c2b",
   "outputs": [],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T16:36:31.308090Z",
     "start_time": "2024-08-22T16:36:30.575162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "def transform_transactional_data(payroll_files):\n",
    "    # Initialize empty DataFrames for dimensions\n",
    "    dim_employee_df = pd.DataFrame(columns=table_schemas['dim_employee'])\n",
    "    dim_agency_df = pd.DataFrame(columns=table_schemas['dim_agency'])\n",
    "    dim_title_df = pd.DataFrame(columns=table_schemas['dim_title'])\n",
    "    \n",
    "    fact_payroll_df = pd.DataFrame(columns=table_schemas['fact_payroll'])\n",
    "\n",
    "    for file_name in payroll_files:\n",
    "        # Extract data from the transactional files\n",
    "        df = extract_from_s3(s3_client, s3_bucket, s3_prefix, file_name)\n",
    "\n",
    "        # Clean and validate the transactional data\n",
    "        df_cleaned = validate_and_clean_transactional_data(df, transaction_columns)\n",
    "\n",
    "        # Update dimension DataFrames\n",
    "        # For dim_employee\n",
    "        dim_employee_data = df_cleaned[['EmployeeID', 'FirstName', 'LastName', 'LeaveStatusasofJune30']]\n",
    "        dim_employee_df = pd.concat([\n",
    "            dim_employee_df,\n",
    "            ensure_columns(dim_employee_data, table_schemas['dim_employee']).drop_duplicates()\n",
    "        ], ignore_index=True)\n",
    "        \n",
    "        # For dim_agency\n",
    "        dim_agency_data = df_cleaned[['AgencyID', 'AgencyName', 'AgencyStartDate']]\n",
    "        dim_agency_df = pd.concat([\n",
    "            dim_agency_df,\n",
    "            ensure_columns(dim_agency_data, table_schemas['dim_agency']).drop_duplicates()\n",
    "        ], ignore_index=True)\n",
    "        \n",
    "        # For dim_title\n",
    "        dim_title_data = df_cleaned[['TitleCode', 'TitleDescription']]\n",
    "        dim_title_df = pd.concat([\n",
    "            dim_title_df,\n",
    "            ensure_columns(dim_title_data, table_schemas['dim_title']).drop_duplicates()\n",
    "        ], ignore_index=True)\n",
    "        \n",
    "        # Prepare fact_payroll DataFrame\n",
    "        df_cleaned['PayrollID'] = range(1, len(df_cleaned) + 1)\n",
    "        fact_payroll_data = df_cleaned[['PayrollID', 'EmployeeID', 'FiscalYear', 'PayrollNumber', 'BaseSalary',\n",
    "                                        'RegularHours', 'RegularGrossPaid', 'OTHours', 'TotalOTPaid', 'TotalOtherPay',\n",
    "                                        'WorkLocationBorough']]\n",
    "        fact_payroll_df = pd.concat([\n",
    "            fact_payroll_df,\n",
    "            ensure_columns(fact_payroll_data, table_schemas['fact_payroll']).drop_duplicates()\n",
    "        ], ignore_index=True)\n",
    "\n",
    "    # Return the dimension DataFrames and fact DataFrame\n",
    "    return dim_employee_df, dim_agency_df, dim_title_df, fact_payroll_df\n",
    "\n",
    "transform_transactional_data(payroll_files)"
   ],
   "id": "89ca08b88f16b9f0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-22 17:36:30,601 - root - INFO - Extracting nycpayroll_2020.csv from S3\n",
      "2024-08-22 17:36:30,808 - root - INFO - Validating and cleaning transactional data\n",
      "2024-08-22 17:36:30,953 - root - INFO - Transactional Data Cleaning Summary: Dropped rows with missing values in FiscalYear as it was <= 5%; Dropped rows with missing values in PayrollNumber as it was <= 5%; Dropped rows with missing values in AgencyID as it was <= 5%; Dropped rows with missing values in AgencyName as it was <= 5%; Dropped rows with missing values in EmployeeID as it was <= 5%; Dropped rows with missing values in LastName as it was <= 5%; Dropped rows with missing values in FirstName as it was <= 5%; Dropped rows with missing values in AgencyStartDate as it was <= 5%; Dropped rows with missing values in WorkLocationBorough as it was <= 5%; Dropped rows with missing values in TitleCode as it was <= 5%; Dropped rows with missing values in TitleDescription as it was <= 5%; Dropped rows with missing values in LeaveStatusasofJune30 as it was <= 5%; Dropped rows with missing values in BaseSalary as it was <= 5%; Dropped rows with missing values in PayBasis as it was <= 5%; Dropped rows with missing values in RegularHours as it was <= 5%; Dropped rows with missing values in RegularGrossPaid as it was <= 5%; Dropped rows with missing values in OTHours as it was <= 5%; Dropped rows with missing values in TotalOTPaid as it was <= 5%; Dropped rows with missing values in TotalOtherPay as it was <= 5%; Converted non-numeric values in EmployeeID to NaN.; Converted non-numeric values in TitleCode to NaN.; Converted non-numeric values in AgencyID to NaN.; Converted non-numeric values in PayrollNumber to NaN.; Replaced outliers in BaseSalary with mean value: 94344.41990000001; Replaced outliers in RegularGrossPaid with mean value: 61417.81569999999; Replaced outliers in OTHours with mean value: 72.78; Replaced outliers in TotalOTPaid with mean value: 3355.0415000000003; Replaced negative values in TotalOtherPay with their positive equivalents.; Replaced outliers in TotalOtherPay with mean value: 5406.189; Replaced outlier FiscalYear values with most frequent year: 2020; Standardized FirstName to title case.; Standardized LastName to title case.; Standardized PayBasis to uppercase.; Standardized WorkLocationBorough to uppercase.; Standardized AgencyStartDate to datetime format.\n",
      "2024-08-22 17:36:30,984 - root - INFO - Extracting nycpayroll_2021.csv from S3\n",
      "2024-08-22 17:36:31,048 - root - INFO - Validating and cleaning transactional data\n",
      "2024-08-22 17:36:31,165 - root - INFO - Transactional Data Cleaning Summary: Dropped rows with missing values in FiscalYear as it was <= 5%; Dropped rows with missing values in PayrollNumber as it was <= 5%; Dropped rows with missing values in AgencyID as it was <= 5%; Dropped rows with missing values in AgencyName as it was <= 5%; Dropped rows with missing values in EmployeeID as it was <= 5%; Dropped rows with missing values in LastName as it was <= 5%; Dropped rows with missing values in FirstName as it was <= 5%; Dropped rows with missing values in AgencyStartDate as it was <= 5%; Dropped rows with missing values in WorkLocationBorough as it was <= 5%; Dropped rows with missing values in TitleCode as it was <= 5%; Dropped rows with missing values in TitleDescription as it was <= 5%; Dropped rows with missing values in LeaveStatusasofJune30 as it was <= 5%; Dropped rows with missing values in BaseSalary as it was <= 5%; Dropped rows with missing values in PayBasis as it was <= 5%; Dropped rows with missing values in RegularHours as it was <= 5%; Dropped rows with missing values in RegularGrossPaid as it was <= 5%; Dropped rows with missing values in OTHours as it was <= 5%; Dropped rows with missing values in TotalOTPaid as it was <= 5%; Dropped rows with missing values in TotalOtherPay as it was <= 5%; Converted non-numeric values in EmployeeID to NaN.; Converted non-numeric values in TitleCode to NaN.; Converted non-numeric values in AgencyID to NaN.; Converted non-numeric values in PayrollNumber to NaN.; Replaced outliers in BaseSalary with mean value: 108447.15782178221; Replaced outliers in RegularHours with mean value: 1941.4059405940593; Replaced outliers in RegularGrossPaid with mean value: 165771.6408910891; Replaced negative values in OTHours with their positive equivalents.; Replaced outliers in OTHours with mean value: 1017.1178217821783; Replaced negative values in TotalOTPaid with their positive equivalents.; Replaced negative values in TotalOtherPay with their positive equivalents.; Replaced outliers in TotalOtherPay with mean value: 30303.672277227717; Replaced outlier FiscalYear values with most frequent year: 2021; Standardized FirstName to title case.; Standardized LastName to title case.; Standardized PayBasis to uppercase.; Standardized WorkLocationBorough to uppercase.; Standardized AgencyStartDate to datetime format.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(    EmployeeID    FirstName         LastName LeaveStatusasofJune30\n",
       " 0        10001     Veronica           Geager                ACTIVE\n",
       " 1       149612     Jonathan            Rotta                ACTIVE\n",
       " 2       206583       Robert        Wilson Ii                ACTIVE\n",
       " 3       199874       Moriah       Washington                ACTIVE\n",
       " 4        58036       Amanda         Krawczyk                ACTIVE\n",
       " ..         ...          ...              ...                   ...\n",
       " 195     164144      Darlene  Massey-Covingto                ACTIVE\n",
       " 196      52830    Archibald          Harding                ACTIVE\n",
       " 197     174178       Joseph          Mcgeary                ACTIVE\n",
       " 198     302174  Christopher            Cheng                ACTIVE\n",
       " 199     229552       Manuel           Ortega                ACTIVE\n",
       " \n",
       " [200 rows x 4 columns],\n",
       "     AgencyID                      AgencyName AgencyStartDate\n",
       " 0       2120  OFFICE OF EMERGENCY MANAGEMENT      2016-09-12\n",
       " 1       2120  OFFICE OF EMERGENCY MANAGEMENT      2013-09-16\n",
       " 2       2120  OFFICE OF EMERGENCY MANAGEMENT      2018-04-30\n",
       " 3       2120  OFFICE OF EMERGENCY MANAGEMENT      2019-03-18\n",
       " 4       2120  OFFICE OF EMERGENCY MANAGEMENT      2017-05-15\n",
       " ..       ...                             ...             ...\n",
       " 179     2017        DEPARTMENT OF CORRECTION      2004-03-04\n",
       " 180     2141     DEPT OF CITYWIDE ADMIN SVCS      2016-05-31\n",
       " 181     2012                 FIRE DEPARTMENT      1990-08-19\n",
       " 182     2010               POLICE DEPARTMENT      1998-06-30\n",
       " 183     2010               POLICE DEPARTMENT      2018-02-12\n",
       " \n",
       " [184 rows x 3 columns],\n",
       "    TitleCode                              TitleDescription\n",
       " 0      40447                EMERGENCY PREPAREDNESS MANAGER\n",
       " 1      40285          COMMISSIONER OF EMERGENCY MANAGEMENT\n",
       " 2      40448             EMERGENCY PREPAREDNESS SPECIALIST\n",
       " 3      40291                           COMMUNITY ASSOCIATE\n",
       " 4      40362                           DEPUTY COMMISSIONER\n",
       " 5      40088                               AGENCY ATTORNEY\n",
       " 6      40494                     FIRST DEPUTY COMMISSIONER\n",
       " 7      40199                                BUDGET ANALYST\n",
       " 8      40409               DIRECTOR OF MANAGEMENT & BUDGET\n",
       " 9      40081                  ADMINISTRATIVE STAFF ANALYST\n",
       " 10     40462                      EXECUTIVE AGENCY COUNSEL\n",
       " 11     40306                      COMPUTER SYSTEMS MANAGER\n",
       " 12     40475                            EXECUTIVE DIRECTOR\n",
       " 13     41143                                         CHAIR\n",
       " 14     40782                           STATIONARY ENGINEER\n",
       " 15     40640                                     PRESIDENT\n",
       " 16     40812                        SUPERVISOR ELECTRICIAN\n",
       " 17     41011                         CITY MEDICAL EXAMINER\n",
       " 18     40237                                 CHIEF ACTUARY\n",
       " 19     40747                    SENIOR STATIONARY ENGINEER\n",
       " 20     40081                  ADMINISTRATIVE STAFF ANALYST\n",
       " 21     40809                                    SUPERVISOR\n",
       " 22     40390                      DIRECTOR OF  INVESTMENTS\n",
       " 23     40628                                       PLUMBER\n",
       " 24     40257                       CITY MEDICAL SPECIALIST\n",
       " 25     40861                                VICE PRESIDENT\n",
       " 26     40614                                         OILER\n",
       " 27     40137                      ASSISTANT SUPERINTENDENT\n",
       " 28     40497                            FIRST DEPUTY MAYOR\n",
       " 29     40241                            CHIEF FIRE MARSHAL\n",
       " 30     40222                                     CARPENTER\n",
       " 31     41125                        GENERAL SUPERINTENDENT\n",
       " 32     41252                   ASSISTANT DISTRICT ATTORNEY\n",
       " 33     40304                           COMPUTER SPECIALIST\n",
       " 34     40092                                       ANALYST\n",
       " 35     40831                            SUPERVISOR PLUMBER\n",
       " 36     41149  SUPERVISING DEPUTY SHERIFF - AL 1 ONLY 40 HR\n",
       " 37     40864  WARDEN-ASSISTANT DEPUTY WARDEN TED < 11/1/92\n",
       " 38     40824                       SUPERVISOR OF MECHANICS\n",
       " 39     40207                                       CAPTAIN\n",
       " 40     40538                            INVESTMENT MANAGER\n",
       " 41     40642                                     PRINCIPAL\n",
       " 42     40440                                   ELECTRICIAN\n",
       " 43     40587                               MARINE ENGINEER\n",
       " 44     40579             LIEUTENANT D/A SPECIAL ASSIGNMENT,\n",
       "     PayrollID EmployeeID FiscalYear PayrollNumber  BaseSalary  RegularHours   \n",
       " 0           1      10001       2020            17     86005.0        1820.0  \\\n",
       " 1           2     149612       2020            17     86005.0        1820.0   \n",
       " 2           3     206583       2020            17     86005.0        1820.0   \n",
       " 3           4     199874       2020            17     86005.0        1820.0   \n",
       " 4           5      58036       2020            17     86005.0        1820.0   \n",
       " ..        ...        ...        ...           ...         ...           ...   \n",
       " 196        97      52830       2021           868       508.8        2080.0   \n",
       " 197        98     174178       2021            57    135511.0        2080.0   \n",
       " 198        99     302174       2021            56    149068.0        2080.0   \n",
       " 199       100     229552       2021            56       508.8        2080.0   \n",
       " 200       101     229552       2021            45       508.8        2080.0   \n",
       " \n",
       "      RegularGrossPaid  OTHours  TotalOTPaid  TotalOtherPay WorkLocationBorough  \n",
       " 0            84698.21     0.00         0.00           0.00            BROOKLYN  \n",
       " 1            84698.21     0.00         0.00           0.00            BROOKLYN  \n",
       " 2            84698.21     0.00         0.00           0.00            BROOKLYN  \n",
       " 3            87900.95     0.00         0.00        3202.74            BROOKLYN  \n",
       " 4            83976.54     0.00         0.00           0.00            BROOKLYN  \n",
       " ..                ...      ...          ...            ...                 ...  \n",
       " 196         132288.00   901.25     90004.07       36369.10           MANHATTAN  \n",
       " 197         146527.13   833.63     77912.32       34206.86            BROOKLYN  \n",
       " 198         147066.63   950.08     90551.25       20495.26           MANHATTAN  \n",
       " 199         131779.20  1085.25    102184.53       24136.67           MANHATTAN  \n",
       " 200         131779.20  1085.25    102184.53       24136.67           MANHATTAN  \n",
       " \n",
       " [201 rows x 11 columns])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 122
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T16:43:36.211410Z",
     "start_time": "2024-08-22T16:43:36.208021Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "278825a2c01ea3c2",
   "outputs": [],
   "execution_count": 122
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ffa5a720f4a4f484"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "94fef79a7091d6db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "89f316fdf18775f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "893636250cb8c7cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1789b1e77272490e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "658804db9eda27d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "725ff1871586dfd9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "148a8663dc0cfb73"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T11:19:34.748221Z",
     "start_time": "2024-08-20T11:19:34.734353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from helpers.db_utils import redshift_engine, stage_data, create_fact_table\n",
    "from helpers.metrics import rows_staged,fact_table_created\n",
    "from sqlalchemy import MetaData, Table, Column, Integer, String\n",
    "import logging\n",
    "\n",
    "metadata = MetaData()\n",
    "engine = redshift_engine()\n",
    "\n",
    "def load_master_data(df, table_name, engine):\n",
    "    logging.info(f\"Loading master data into {table_name}\")\n",
    "    stage_data(engine, df, table_name)\n",
    "    rows_staged.set(len(df))\n",
    "\n",
    "def load_transactional_data(df, engine):\n",
    "    logging.info(f\"Loading transactional data into FactPayroll\")\n",
    "    create_fact_table(engine, engine.metadata)\n",
    "    fact_table_created.set(1)\n",
    "    stage_data(engine, df, 'FactPayroll')\n",
    "    rows_staged.set(len(df))\n"
   ],
   "id": "a67c5202c036d3ed",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "def ingest_master_data():\n",
    "\n",
    "\n",
    "    for file_name, table_name, dim_col in zip(master_files, master_table_names, dim_columns):\n",
    "        df = extract.extract_from_s3(s3_bucket, s3_prefix, file_name)\n",
    "        df_transformed = transform.transform_master_data(df, dim_col)\n",
    "        stage_data(df_transformed, table_name)\n",
    "\n",
    "\n",
    "def ingest_transactional_data():\n",
    "    logging.info(f\"Ingesting transactional data\")\n",
    "    s3_bucket = 'your-s3-bucket-name'\n",
    "    s3_prefix = 'your-folder-prefix/'\n",
    "    payroll_files = ['nycpayroll_2020.csv', 'nycpayroll_2021.csv']\n",
    "\n",
    "    create_fact_table()\n",
    "\n",
    "    for file_name in payroll_files:\n",
    "        df = extract.extract_from_s3(s3_bucket, s3_prefix, file_name)\n",
    "        df_transformed = transform.transform_transactional_data(df, engine)\n",
    "        stage_data(df_transformed, 'FactPayroll')"
   ],
   "id": "bb4885ce4271e343"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
