{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afcb1b4f553ebfa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T21:27:12.842500Z",
     "start_time": "2024-08-21T21:27:12.706647Z"
    }
   },
   "outputs": [],
   "source": [
    "from helpers.metrics_server import start_metrics_server, files_extracted, rows_extracted, rows_transformed, rows_validated, missing_values_detected, rows_staged, rows_processed, rows_cleaned, data_quality_issues \n",
    "from helpers.logging_utils import setup_logging\n",
    "\n",
    "setup_logging()\n",
    "load_dotenv(override=True)\n",
    "\n",
    "start_metrics_server(port=8002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93f99e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-30 19:38:22,897 - root - INFO - Extracting AgencyMaster.csv from S3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   AgencyID                      AgencyName\n",
      "0      2001       ADMIN FOR CHILDREN'S SVCS\n",
      "1      2002       ADMIN TRIALS AND HEARINGS\n",
      "2      2003             BOARD OF CORRECTION\n",
      "3      2004               BOARD OF ELECTION\n",
      "4      2005  BOARD OF ELECTION POLL WORKERS\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from scripts.extract import extract_data\n",
    "from helpers.s3_utils import get_s3_client\n",
    "\n",
    "# Initialize S3 client and other variables\n",
    "s3_client = get_s3_client()\n",
    "s3_bucket = os.getenv(\"s3_bucket\")\n",
    "s3_prefix = os.getenv(\"s3_prefix\")\n",
    "\n",
    "# Call extract_data with the necessary parameters\n",
    "df = extract_data('AgencyMaster.csv', s3_client, s3_bucket, s3_prefix)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30b2e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.db_utils import redshift_engine, stage_data\n",
    "\n",
    "engine = redshift_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c036fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-30 19:38:27,974 - root - INFO - Validating and cleaning master data\n",
      "2024-08-30 19:38:27,982 - root - INFO - Master Data Cleaning Summary: Converted non-numeric values in AgencyID to NaN.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from scripts.validate import validate_and_clean_master_data, validate_and_clean_transactional_data\n",
    "\n",
    "attributes = [\n",
    "    'FiscalYear', 'PayrollNumber', 'AgencyID', 'AgencyName', 'EmployeeID', 'LastName', 'FirstName',\n",
    "    'AgencyStartDate', 'WorkLocationBorough', 'TitleCode', 'TitleDescription', 'LeaveStatusasofJune30',\n",
    "    'BaseSalary', 'PayBasis', 'RegularHours', 'RegularGrossPaid', 'OTHours', 'TotalOTPaid', 'TotalOtherPay'\n",
    "]\n",
    "\n",
    "df = validate_and_clean_master_data(df, attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfff9b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AgencyID</th>\n",
       "      <th>AgencyName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001</td>\n",
       "      <td>ADMIN FOR CHILDREN'S SVCS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2002</td>\n",
       "      <td>ADMIN TRIALS AND HEARINGS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003</td>\n",
       "      <td>BOARD OF CORRECTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004</td>\n",
       "      <td>BOARD OF ELECTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2005</td>\n",
       "      <td>BOARD OF ELECTION POLL WORKERS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AgencyID                      AgencyName\n",
       "0      2001       ADMIN FOR CHILDREN'S SVCS\n",
       "1      2002       ADMIN TRIALS AND HEARINGS\n",
       "2      2003             BOARD OF CORRECTION\n",
       "3      2004               BOARD OF ELECTION\n",
       "4      2005  BOARD OF ELECTION POLL WORKERS"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e433dad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-30 19:38:46,547 - root - INFO - Extracting nycpayroll_2021.csv from S3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FiscalYear</th>\n",
       "      <th>PayrollNumber</th>\n",
       "      <th>AgencyCode</th>\n",
       "      <th>AgencyName</th>\n",
       "      <th>EmployeeID</th>\n",
       "      <th>LastName</th>\n",
       "      <th>FirstName</th>\n",
       "      <th>AgencyStartDate</th>\n",
       "      <th>WorkLocationBorough</th>\n",
       "      <th>TitleCode</th>\n",
       "      <th>TitleDescription</th>\n",
       "      <th>LeaveStatusasofJune30</th>\n",
       "      <th>BaseSalary</th>\n",
       "      <th>PayBasis</th>\n",
       "      <th>RegularHours</th>\n",
       "      <th>RegularGrossPaid</th>\n",
       "      <th>OTHours</th>\n",
       "      <th>TotalOTPaid</th>\n",
       "      <th>TotalOtherPay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021</td>\n",
       "      <td>996</td>\n",
       "      <td>2153</td>\n",
       "      <td>NYC HOUSING AUTHORITY</td>\n",
       "      <td>209184</td>\n",
       "      <td>MUSTACIUOLO</td>\n",
       "      <td>VITO</td>\n",
       "      <td>2/26/2018</td>\n",
       "      <td>MANHATTAN</td>\n",
       "      <td>40475</td>\n",
       "      <td>EXECUTIVE DIRECTOR</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>258000.0</td>\n",
       "      <td>per Annum</td>\n",
       "      <td>1820</td>\n",
       "      <td>257260.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>258000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021</td>\n",
       "      <td>996</td>\n",
       "      <td>2153</td>\n",
       "      <td>NYC HOUSING AUTHORITY</td>\n",
       "      <td>302330</td>\n",
       "      <td>RUSS</td>\n",
       "      <td>GREGORY</td>\n",
       "      <td>8/12/2019</td>\n",
       "      <td>MANHATTAN</td>\n",
       "      <td>41143</td>\n",
       "      <td>CHAIR</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>414707.0</td>\n",
       "      <td>per Annum</td>\n",
       "      <td>1820</td>\n",
       "      <td>413518.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>500.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021</td>\n",
       "      <td>816</td>\n",
       "      <td>2129</td>\n",
       "      <td>DEPT OF HEALTH/MENTAL HYGIENE</td>\n",
       "      <td>49788</td>\n",
       "      <td>HALLAHAN</td>\n",
       "      <td>PATRICK</td>\n",
       "      <td>2/26/2018</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>40782</td>\n",
       "      <td>STATIONARY ENGINEER</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>508.8</td>\n",
       "      <td>per Day</td>\n",
       "      <td>2080</td>\n",
       "      <td>132288.00</td>\n",
       "      <td>2115.25</td>\n",
       "      <td>218628.18</td>\n",
       "      <td>56616.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021</td>\n",
       "      <td>816</td>\n",
       "      <td>2129</td>\n",
       "      <td>DEPT OF HEALTH/MENTAL HYGIENE</td>\n",
       "      <td>251626</td>\n",
       "      <td>PETTIT</td>\n",
       "      <td>PATRICK</td>\n",
       "      <td>8/2/2010</td>\n",
       "      <td>MANHATTAN</td>\n",
       "      <td>40782</td>\n",
       "      <td>STATIONARY ENGINEER</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>508.8</td>\n",
       "      <td>per Day</td>\n",
       "      <td>2080</td>\n",
       "      <td>132288.00</td>\n",
       "      <td>2152.75</td>\n",
       "      <td>218694.96</td>\n",
       "      <td>38611.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021</td>\n",
       "      <td>816</td>\n",
       "      <td>2129</td>\n",
       "      <td>DEPT OF HEALTH/MENTAL HYGIENE</td>\n",
       "      <td>364376</td>\n",
       "      <td>TELEHANY</td>\n",
       "      <td>STEPHEN</td>\n",
       "      <td>1/16/2007</td>\n",
       "      <td>QUEENS</td>\n",
       "      <td>40782</td>\n",
       "      <td>STATIONARY ENGINEER</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>508.8</td>\n",
       "      <td>per Day</td>\n",
       "      <td>2080</td>\n",
       "      <td>132288.00</td>\n",
       "      <td>1876.25</td>\n",
       "      <td>192296.19</td>\n",
       "      <td>51160.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FiscalYear  PayrollNumber  AgencyCode                     AgencyName   \n",
       "0        2021            996        2153          NYC HOUSING AUTHORITY  \\\n",
       "1        2021            996        2153          NYC HOUSING AUTHORITY   \n",
       "2        2021            816        2129  DEPT OF HEALTH/MENTAL HYGIENE   \n",
       "3        2021            816        2129  DEPT OF HEALTH/MENTAL HYGIENE   \n",
       "4        2021            816        2129  DEPT OF HEALTH/MENTAL HYGIENE   \n",
       "\n",
       "   EmployeeID     LastName FirstName AgencyStartDate WorkLocationBorough   \n",
       "0      209184  MUSTACIUOLO      VITO       2/26/2018           MANHATTAN  \\\n",
       "1      302330         RUSS   GREGORY       8/12/2019           MANHATTAN   \n",
       "2       49788     HALLAHAN   PATRICK       2/26/2018            BROOKLYN   \n",
       "3      251626       PETTIT   PATRICK        8/2/2010           MANHATTAN   \n",
       "4      364376     TELEHANY   STEPHEN       1/16/2007              QUEENS   \n",
       "\n",
       "   TitleCode     TitleDescription LeaveStatusasofJune30  BaseSalary   \n",
       "0      40475   EXECUTIVE DIRECTOR                ACTIVE    258000.0  \\\n",
       "1      41143                CHAIR                ACTIVE    414707.0   \n",
       "2      40782  STATIONARY ENGINEER                ACTIVE       508.8   \n",
       "3      40782  STATIONARY ENGINEER                ACTIVE       508.8   \n",
       "4      40782  STATIONARY ENGINEER                ACTIVE       508.8   \n",
       "\n",
       "    PayBasis  RegularHours  RegularGrossPaid  OTHours  TotalOTPaid   \n",
       "0  per Annum          1820         257260.30     0.00         0.00  \\\n",
       "1  per Annum          1820         413518.05     0.00         0.00   \n",
       "2    per Day          2080         132288.00  2115.25    218628.18   \n",
       "3    per Day          2080         132288.00  2152.75    218694.96   \n",
       "4    per Day          2080         132288.00  1876.25    192296.19   \n",
       "\n",
       "   TotalOtherPay  \n",
       "0      258000.00  \n",
       "1         500.00  \n",
       "2       56616.07  \n",
       "3       38611.82  \n",
       "4       51160.20  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = extract_data('nycpayroll_2021.csv', s3_client, s3_bucket, s3_prefix)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bc615d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-30 19:39:13,913 - root - INFO - Validating and cleaning transactional data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-30 19:39:14,134 - root - INFO - Transactional Data Cleaning Summary: Dropped rows with missing values in FiscalYear as it was <= 5%; Dropped rows with missing values in PayrollNumber as it was <= 5%; Dropped rows with missing values in AgencyID as it was <= 5%; Dropped rows with missing values in AgencyName as it was <= 5%; Dropped rows with missing values in EmployeeID as it was <= 5%; Dropped rows with missing values in LastName as it was <= 5%; Dropped rows with missing values in FirstName as it was <= 5%; Dropped rows with missing values in AgencyStartDate as it was <= 5%; Dropped rows with missing values in WorkLocationBorough as it was <= 5%; Dropped rows with missing values in TitleCode as it was <= 5%; Dropped rows with missing values in TitleDescription as it was <= 5%; Dropped rows with missing values in LeaveStatusasofJune30 as it was <= 5%; Dropped rows with missing values in BaseSalary as it was <= 5%; Dropped rows with missing values in PayBasis as it was <= 5%; Dropped rows with missing values in RegularHours as it was <= 5%; Dropped rows with missing values in RegularGrossPaid as it was <= 5%; Dropped rows with missing values in OTHours as it was <= 5%; Dropped rows with missing values in TotalOTPaid as it was <= 5%; Dropped rows with missing values in TotalOtherPay as it was <= 5%; Converted non-numeric values in EmployeeID to NaN.; Converted non-numeric values in TitleCode to NaN.; Converted non-numeric values in AgencyID to NaN.; Converted non-numeric values in PayrollNumber to NaN.; Replaced negative values in OTHours with their positive equivalents.; Replaced negative values in TotalOTPaid with their positive equivalents.; Replaced negative values in TotalOtherPay with their positive equivalents.; Standardized FirstName to title case.; Standardized LastName to title case.; Standardized PayBasis to uppercase.; Standardized WorkLocationBorough to uppercase.; Standardized AgencyStartDate to datetime format.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FiscalYear</th>\n",
       "      <th>PayrollNumber</th>\n",
       "      <th>AgencyID</th>\n",
       "      <th>AgencyName</th>\n",
       "      <th>EmployeeID</th>\n",
       "      <th>LastName</th>\n",
       "      <th>FirstName</th>\n",
       "      <th>AgencyStartDate</th>\n",
       "      <th>WorkLocationBorough</th>\n",
       "      <th>TitleCode</th>\n",
       "      <th>TitleDescription</th>\n",
       "      <th>LeaveStatusasofJune30</th>\n",
       "      <th>BaseSalary</th>\n",
       "      <th>PayBasis</th>\n",
       "      <th>RegularHours</th>\n",
       "      <th>RegularGrossPaid</th>\n",
       "      <th>OTHours</th>\n",
       "      <th>TotalOTPaid</th>\n",
       "      <th>TotalOtherPay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021</td>\n",
       "      <td>996</td>\n",
       "      <td>2153</td>\n",
       "      <td>NYC HOUSING AUTHORITY</td>\n",
       "      <td>209184</td>\n",
       "      <td>Mustaciuolo</td>\n",
       "      <td>Vito</td>\n",
       "      <td>2018-02-26</td>\n",
       "      <td>MANHATTAN</td>\n",
       "      <td>40475</td>\n",
       "      <td>EXECUTIVE DIRECTOR</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>258000.0</td>\n",
       "      <td>PER ANNUM</td>\n",
       "      <td>1820</td>\n",
       "      <td>257260.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>258000.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021</td>\n",
       "      <td>996</td>\n",
       "      <td>2153</td>\n",
       "      <td>NYC HOUSING AUTHORITY</td>\n",
       "      <td>302330</td>\n",
       "      <td>Russ</td>\n",
       "      <td>Gregory</td>\n",
       "      <td>2019-08-12</td>\n",
       "      <td>MANHATTAN</td>\n",
       "      <td>41143</td>\n",
       "      <td>CHAIR</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>414707.0</td>\n",
       "      <td>PER ANNUM</td>\n",
       "      <td>1820</td>\n",
       "      <td>413518.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>500.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021</td>\n",
       "      <td>816</td>\n",
       "      <td>2129</td>\n",
       "      <td>DEPT OF HEALTH/MENTAL HYGIENE</td>\n",
       "      <td>49788</td>\n",
       "      <td>Hallahan</td>\n",
       "      <td>Patrick</td>\n",
       "      <td>2018-02-26</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>40782</td>\n",
       "      <td>STATIONARY ENGINEER</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>508.8</td>\n",
       "      <td>PER DAY</td>\n",
       "      <td>2080</td>\n",
       "      <td>132288.00</td>\n",
       "      <td>2115.25</td>\n",
       "      <td>218628.18</td>\n",
       "      <td>56616.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021</td>\n",
       "      <td>816</td>\n",
       "      <td>2129</td>\n",
       "      <td>DEPT OF HEALTH/MENTAL HYGIENE</td>\n",
       "      <td>251626</td>\n",
       "      <td>Pettit</td>\n",
       "      <td>Patrick</td>\n",
       "      <td>2010-08-02</td>\n",
       "      <td>MANHATTAN</td>\n",
       "      <td>40782</td>\n",
       "      <td>STATIONARY ENGINEER</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>508.8</td>\n",
       "      <td>PER DAY</td>\n",
       "      <td>2080</td>\n",
       "      <td>132288.00</td>\n",
       "      <td>2152.75</td>\n",
       "      <td>218694.96</td>\n",
       "      <td>38611.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021</td>\n",
       "      <td>816</td>\n",
       "      <td>2129</td>\n",
       "      <td>DEPT OF HEALTH/MENTAL HYGIENE</td>\n",
       "      <td>364376</td>\n",
       "      <td>Telehany</td>\n",
       "      <td>Stephen</td>\n",
       "      <td>2007-01-16</td>\n",
       "      <td>QUEENS</td>\n",
       "      <td>40782</td>\n",
       "      <td>STATIONARY ENGINEER</td>\n",
       "      <td>ACTIVE</td>\n",
       "      <td>508.8</td>\n",
       "      <td>PER DAY</td>\n",
       "      <td>2080</td>\n",
       "      <td>132288.00</td>\n",
       "      <td>1876.25</td>\n",
       "      <td>192296.19</td>\n",
       "      <td>51160.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FiscalYear  PayrollNumber  AgencyID                     AgencyName   \n",
       "0        2021            996      2153          NYC HOUSING AUTHORITY  \\\n",
       "1        2021            996      2153          NYC HOUSING AUTHORITY   \n",
       "2        2021            816      2129  DEPT OF HEALTH/MENTAL HYGIENE   \n",
       "3        2021            816      2129  DEPT OF HEALTH/MENTAL HYGIENE   \n",
       "4        2021            816      2129  DEPT OF HEALTH/MENTAL HYGIENE   \n",
       "\n",
       "   EmployeeID     LastName FirstName AgencyStartDate WorkLocationBorough   \n",
       "0      209184  Mustaciuolo      Vito      2018-02-26           MANHATTAN  \\\n",
       "1      302330         Russ   Gregory      2019-08-12           MANHATTAN   \n",
       "2       49788     Hallahan   Patrick      2018-02-26            BROOKLYN   \n",
       "3      251626       Pettit   Patrick      2010-08-02           MANHATTAN   \n",
       "4      364376     Telehany   Stephen      2007-01-16              QUEENS   \n",
       "\n",
       "   TitleCode     TitleDescription LeaveStatusasofJune30  BaseSalary   \n",
       "0      40475   EXECUTIVE DIRECTOR                ACTIVE    258000.0  \\\n",
       "1      41143                CHAIR                ACTIVE    414707.0   \n",
       "2      40782  STATIONARY ENGINEER                ACTIVE       508.8   \n",
       "3      40782  STATIONARY ENGINEER                ACTIVE       508.8   \n",
       "4      40782  STATIONARY ENGINEER                ACTIVE       508.8   \n",
       "\n",
       "    PayBasis  RegularHours  RegularGrossPaid  OTHours  TotalOTPaid   \n",
       "0  PER ANNUM          1820         257260.30     0.00         0.00  \\\n",
       "1  PER ANNUM          1820         413518.05     0.00         0.00   \n",
       "2    PER DAY          2080         132288.00  2115.25    218628.18   \n",
       "3    PER DAY          2080         132288.00  2152.75    218694.96   \n",
       "4    PER DAY          2080         132288.00  1876.25    192296.19   \n",
       "\n",
       "   TotalOtherPay  \n",
       "0      258000.00  \n",
       "1         500.00  \n",
       "2       56616.07  \n",
       "3       38611.82  \n",
       "4       51160.20  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = validate_and_clean_transactional_data(df, attributes)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8fdeb1",
   "metadata": {},
   "source": [
    "# Transformation and Ingest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17beb2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import logging\n",
    "from helpers.logging_utils import setup_logging\n",
    "\n",
    "from helpers.db_utils import stage_data\n",
    "from scripts.transform_ingest import ensure_columns, transform_master_data, transform_transactional_data\n",
    "from scripts.extract import extract_data\n",
    "from helpers.s3_utils import get_s3_client\n",
    "from scripts.validate import validate_and_clean_master_data, validate_and_clean_transactional_data\n",
    "from helpers.db_utils import redshift_engine, stage_data \n",
    "from helpers.metrics_server import start_metrics_server, files_extracted, rows_extracted, rows_transformed, rows_validated, missing_values_detected, rows_staged, rows_processed, rows_cleaned, data_quality_issues \n",
    "\n",
    "\n",
    "setup_logging()\n",
    "\n",
    "def transform_master_data(master_files):\n",
    "    \"\"\"\n",
    "    Transform and stage master data from given files into dimension tables.\n",
    "    \n",
    "    Args:\n",
    "        master_files (list of str): List of file names containing master data.\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initialize empty DataFrames with the required columns based on table schemas\n",
    "    dim_employee_df = pd.DataFrame(columns=table_schemas['dim_employee'])\n",
    "    dim_agency_df = pd.DataFrame(columns=table_schemas['dim_agency'])\n",
    "    dim_title_df = pd.DataFrame(columns=table_schemas['dim_title'])\n",
    "\n",
    "    # Define a mapping from file names to dimension table names and their schemas\n",
    "    file_to_table_map = {\n",
    "        'EmpMaster.csv': ('dim_employee', table_schemas['dim_employee']),\n",
    "        'AgencyMaster.csv': ('dim_agency', table_schemas['dim_agency']),\n",
    "        'TitleMaster.csv': ('dim_title', table_schemas['dim_title'])\n",
    "    }\n",
    "\n",
    "    for file_name in master_files:\n",
    "        table_name, required_columns = file_to_table_map.get(file_name, (None, None))\n",
    "\n",
    "        if table_name:\n",
    "            try:\n",
    "                # Extract data from the file\n",
    "                df = extract_data(file_name, s3_client, s3_bucket, s3_prefix)\n",
    "\n",
    "                # Validate and clean the master data\n",
    "                df_cleaned = validate_and_clean_master_data(df, required_columns)\n",
    "\n",
    "                # Ensure the DataFrame has all required columns\n",
    "                df_cleaned = ensure_columns(df_cleaned, required_columns)\n",
    "\n",
    "                # Append data to the appropriate dimension DataFrame\n",
    "                if table_name == 'dim_employee':\n",
    "                    dim_employee_df = pd.concat([\n",
    "                        dim_employee_df,\n",
    "                        df_cleaned.drop_duplicates()\n",
    "                    ], ignore_index=True)\n",
    "\n",
    "                elif table_name == 'dim_agency':\n",
    "                    dim_agency_df = pd.concat([\n",
    "                        dim_agency_df,\n",
    "                        df_cleaned.drop_duplicates()\n",
    "                    ], ignore_index=True)\n",
    "\n",
    "                elif table_name == 'dim_title':\n",
    "                    dim_title_df = pd.concat([\n",
    "                        dim_title_df,\n",
    "                        df_cleaned.drop_duplicates()\n",
    "                    ], ignore_index=True)\n",
    "            \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing file {file_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "    # Stage the data into the database\n",
    "    try:\n",
    "        stage_data(engine, dim_employee_df, 'dim_employee')\n",
    "        stage_data(engine, dim_agency_df, 'dim_agency')\n",
    "        stage_data(engine, dim_title_df, 'dim_title')\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error staging data: {e}\")\n",
    "        return\n",
    "\n",
    "    total_master_rows = len(dim_employee_df) + len(dim_agency_df) + len(dim_title_df)\n",
    "\n",
    "    # Update Prometheus metrics\n",
    "    rows_transformed.set(total_master_rows)\n",
    "    rows_staged.set(total_master_rows)\n",
    "\n",
    "    # Log success message with details\n",
    "    logging.info(\"Master data successfully transformed and staged.\")\n",
    "    logging.info(f\" - dim_employee: {len(dim_employee_df)} rows\")\n",
    "    logging.info(f\" - dim_agency: {len(dim_agency_df)} rows\")\n",
    "    logging.info(f\" - dim_title: {len(dim_title_df)} rows\")\n",
    "    logging.info(f\"Total master data staged: {total_master_rows} rows\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "master_files = ['EmpMaster.csv', 'TitleMaster.csv', 'AgencyMaster.csv']\n",
    "\n",
    "table_schemas = {\n",
    "    'dim_employee': ['EmployeeID', 'FirstName', 'LastName', 'LeaveStatusasofJune30'],\n",
    "    'dim_agency': ['AgencyID', 'AgencyName', 'AgencyStartDate'],\n",
    "    'dim_title': ['TitleCode', 'TitleDescription'],\n",
    "    'fact_payroll': ['PayrollNumber', 'EmployeeID','AgencyID', 'TitleCode','FiscalYear', 'BaseSalary', 'RegularHours',\n",
    "                     'RegularGrossPaid', 'OTHours', 'TotalOTPaid', 'TotalOtherPay', 'WorkLocationBorough']\n",
    "}\n",
    "\n",
    "attributes = [\n",
    "    'FiscalYear', 'PayrollNumber', 'AgencyID', 'AgencyName', 'EmployeeID', 'LastName', 'FirstName',\n",
    "    'AgencyStartDate', 'WorkLocationBorough', 'TitleCode', 'TitleDescription', 'LeaveStatusasofJune30',\n",
    "    'BaseSalary', 'PayBasis', 'RegularHours', 'RegularGrossPaid', 'OTHours', 'TotalOTPaid', 'TotalOtherPay'\n",
    "]\n",
    "dim_table_names = ['dim_employee', 'dim_title', 'dim_agency']\n",
    "\n",
    "s3_client = get_s3_client()\n",
    "engine = redshift_engine()\n",
    "setup_logging()\n",
    "\n",
    "s3_bucket = os.getenv(\"s3_bucket\")\n",
    "s3_prefix = os.getenv(\"s3_prefix\")\n",
    "\n",
    "transform_master_data(master_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee8693a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import logging\n",
    "from helpers.logging_utils import setup_logging\n",
    "\n",
    "from helpers.db_utils import stage_data\n",
    "from scripts.transform_ingest import ensure_columns, transform_master_data, transform_transactional_data\n",
    "from scripts.extract import extract_data\n",
    "from helpers.s3_utils import get_s3_client\n",
    "from scripts.validate import validate_and_clean_master_data, validate_and_clean_transactional_data\n",
    "from helpers.db_utils import redshift_engine, stage_data \n",
    "from helpers.metrics_server import start_metrics_server, files_extracted, rows_extracted, rows_transformed, rows_validated, missing_values_detected, rows_staged, rows_processed, rows_cleaned, data_quality_issues \n",
    "\n",
    "\n",
    "def transform_transactional_data(payroll_files):\n",
    "    \"\"\"\n",
    "    Transform and stage transactional data from the given payroll files into dimension and fact tables.\n",
    "\n",
    "    Args:\n",
    "        payroll_files (list of str): List of payroll file names to process.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initialize empty DataFrames with the required columns based on table schemas\n",
    "    dim_employee_df = pd.DataFrame(columns=table_schemas['dim_employee'])\n",
    "    dim_agency_df = pd.DataFrame(columns=table_schemas['dim_agency'])\n",
    "    dim_title_df = pd.DataFrame(columns=table_schemas['dim_title'])\n",
    "    fact_payroll_df = pd.DataFrame(columns=table_schemas['fact_payroll'])\n",
    "\n",
    "    for file_name in payroll_files:\n",
    "        try:\n",
    "            # Extract data from the transactional files\n",
    "            df = extract_data(file_name, s3_client, s3_bucket, s3_prefix)\n",
    "\n",
    "            # Clean and validate the transactional data\n",
    "            df_cleaned = validate_and_clean_transactional_data(df, attributes)\n",
    "\n",
    "            # Update dimension DataFrames\n",
    "            # For dim_employee\n",
    "            dim_employee_data = df_cleaned[['EmployeeID', 'FirstName', 'LastName', 'LeaveStatusasofJune30']]\n",
    "            dim_employee_df = pd.concat([\n",
    "                dim_employee_df,\n",
    "                ensure_columns(dim_employee_data, table_schemas['dim_employee']).drop_duplicates()\n",
    "            ], ignore_index=True)\n",
    "            \n",
    "            # For dim_agency\n",
    "            dim_agency_data = df_cleaned[['AgencyID', 'AgencyName', 'AgencyStartDate']]\n",
    "            dim_agency_df = pd.concat([\n",
    "                dim_agency_df,\n",
    "                ensure_columns(dim_agency_data, table_schemas['dim_agency']).drop_duplicates()\n",
    "            ], ignore_index=True)\n",
    "            \n",
    "            # For dim_title\n",
    "            dim_title_data = df_cleaned[['TitleCode', 'TitleDescription']]\n",
    "            dim_title_df = pd.concat([\n",
    "                dim_title_df,\n",
    "                ensure_columns(dim_title_data, table_schemas['dim_title']).drop_duplicates()\n",
    "            ], ignore_index=True)\n",
    "            \n",
    "            # Prepare fact_payroll DataFrame\n",
    "            fact_payroll_data = df_cleaned[['PayrollNumber', 'EmployeeID', 'AgencyID', 'TitleCode','FiscalYear', 'BaseSalary',\n",
    "                                            'RegularHours', 'RegularGrossPaid', 'OTHours', 'TotalOTPaid', 'TotalOtherPay',\n",
    "                                            'WorkLocationBorough']]\n",
    "            fact_payroll_df = pd.concat([\n",
    "                fact_payroll_df,\n",
    "                ensure_columns(fact_payroll_data, table_schemas['fact_payroll']).drop_duplicates()\n",
    "            ], ignore_index=True)\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing file {file_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Stage the data into the database\n",
    "    try:\n",
    "        stage_data(engine, dim_employee_df, 'dim_employee')\n",
    "        stage_data(engine, dim_agency_df, 'dim_agency')\n",
    "        stage_data(engine, dim_title_df, 'dim_title')\n",
    "        stage_data(engine, fact_payroll_df, 'fact_payroll')\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error staging data: {e}\")\n",
    "        return\n",
    "\n",
    "    # Calculate total rows staged\n",
    "    total_transactional_rows = len(fact_payroll_df)\n",
    "    #total_rows = total_master_rows + total_transactional_rows\n",
    "\n",
    "    # Update Prometheus metrics\n",
    "    rows_transformed.set(len(fact_payroll_df))\n",
    "    rows_staged.set(total_transactional_rows)\n",
    "\n",
    "    # Log success message with details\n",
    "    logging.info(\"Transactional data successfully transformed and staged.\")\n",
    "    logging.info(f\" - fact_payroll: {total_transactional_rows} rows\")\n",
    "    logging.info(f\"Total transactional data staged: {total_transactional_rows} rows\")\n",
    "    logging.info(\"All data successfully transformed and staged.\")\n",
    "    #logging.info(f\" - Total master data: {total_master_rows} rows\")  # Ensure total_master_rows is defined\n",
    "    logging.info(f\" - Total transactional data: {total_transactional_rows} rows\")\n",
    "    #logging.info(f\"Total data staged: {total_rows} rows\")\n",
    "\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "master_files = ['EmpMaster.csv', 'TitleMaster.csv', 'AgencyMaster.csv']\n",
    "\n",
    "payroll_files = ['nycpayroll_2021.csv','nycpayroll_2020.csv']\n",
    "\n",
    "table_schemas = {\n",
    "    'dim_employee': ['EmployeeID', 'FirstName', 'LastName', 'LeaveStatusasofJune30'],\n",
    "    'dim_agency': ['AgencyID', 'AgencyName', 'AgencyStartDate'],\n",
    "    'dim_title': ['TitleCode', 'TitleDescription'],\n",
    "    'fact_payroll': ['PayrollNumber', 'EmployeeID','AgencyID', 'TitleCode','FiscalYear', 'BaseSalary', 'RegularHours',\n",
    "                     'RegularGrossPaid', 'OTHours', 'TotalOTPaid', 'TotalOtherPay', 'WorkLocationBorough']\n",
    "}\n",
    "\n",
    "attributes = [\n",
    "    'FiscalYear', 'PayrollNumber', 'AgencyID', 'AgencyName', 'EmployeeID', 'LastName', 'FirstName',\n",
    "    'AgencyStartDate', 'WorkLocationBorough', 'TitleCode', 'TitleDescription', 'LeaveStatusasofJune30',\n",
    "    'BaseSalary', 'PayBasis', 'RegularHours', 'RegularGrossPaid', 'OTHours', 'TotalOTPaid', 'TotalOtherPay'\n",
    "]\n",
    "dim_table_names = ['dim_employee', 'dim_title', 'dim_agency']\n",
    "\n",
    "s3_client = get_s3_client()\n",
    "engine = redshift_engine()\n",
    "setup_logging()\n",
    "\n",
    "s3_bucket = os.getenv(\"s3_bucket\")\n",
    "s3_prefix = os.getenv(\"s3_prefix\")\n",
    "\n",
    "\n",
    "transform_transactional_data(payroll_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7baf78ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to extract <botocore.client.S3 object at 0x11b77e190>: 'str' object has no attribute 'get_object'\n",
      "ERROR:root:Error processing file nycpayroll_2021.csv: 'str' object has no attribute 'get_object'\n",
      "ERROR:root:Failed to extract <botocore.client.S3 object at 0x11b77e190>: 'str' object has no attribute 'get_object'\n",
      "ERROR:root:Error processing file nycpayroll_2020.csv: 'str' object has no attribute 'get_object'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from helpers.logging_utils import setup_logging\n",
    "from helpers.db_utils import stage_data, redshift_engine\n",
    "from scripts.transform_ingest import ensure_columns, transform_master_data, transform_transactional_data\n",
    "from scripts.extract import extract_data\n",
    "from scripts.validate import validate_and_clean_master_data, validate_and_clean_transactional_data\n",
    "from helpers.s3_utils import get_s3_client\n",
    "from helpers.metrics_server import (\n",
    "    start_metrics_server, files_extracted, rows_extracted, rows_transformed, rows_validated, \n",
    "    missing_values_detected, rows_staged, rows_processed, rows_cleaned, data_quality_issues\n",
    ")\n",
    "\n",
    "def transform_transactional_data(payroll_files):\n",
    "    \"\"\"\n",
    "    Transform and stage transactional data from the given payroll files into dimension and fact tables.\n",
    "\n",
    "    Args:\n",
    "        payroll_files (list of str): List of payroll file names to process.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initialize empty DataFrames with the required columns based on table schemas\n",
    "    dim_employee_df = pd.DataFrame(columns=table_schemas['dim_employee'])\n",
    "    dim_agency_df = pd.DataFrame(columns=table_schemas['dim_agency'])\n",
    "    dim_title_df = pd.DataFrame(columns=table_schemas['dim_title'])\n",
    "    fact_payroll_df = pd.DataFrame(columns=table_schemas['fact_payroll'])\n",
    "\n",
    "    for file_name in payroll_files:\n",
    "        try:\n",
    "            # Extract data from the transactional files\n",
    "            df = extract_data(s3_client, s3_bucket, s3_prefix, file_name)\n",
    "\n",
    "            # Clean and validate the transactional data\n",
    "            df_cleaned = validate_and_clean_transactional_data(df, attributes)\n",
    "\n",
    "            # Update dimension DataFrames\n",
    "            # For dim_employee\n",
    "            dim_employee_data = df_cleaned[['EmployeeID', 'FirstName', 'LastName', 'LeaveStatusasofJune30']]\n",
    "            dim_employee_df = pd.concat([\n",
    "                dim_employee_df,\n",
    "                ensure_columns(dim_employee_data, table_schemas['dim_employee']).drop_duplicates()\n",
    "            ], ignore_index=True)\n",
    "            \n",
    "            # For dim_agency\n",
    "            dim_agency_data = df_cleaned[['AgencyID', 'AgencyName', 'AgencyStartDate']]\n",
    "            dim_agency_df = pd.concat([\n",
    "                dim_agency_df,\n",
    "                ensure_columns(dim_agency_data, table_schemas['dim_agency']).drop_duplicates()\n",
    "            ], ignore_index=True)\n",
    "            \n",
    "            # For dim_title\n",
    "            dim_title_data = df_cleaned[['TitleCode', 'TitleDescription']]\n",
    "            dim_title_df = pd.concat([\n",
    "                dim_title_df,\n",
    "                ensure_columns(dim_title_data, table_schemas['dim_title']).drop_duplicates()\n",
    "            ], ignore_index=True)\n",
    "            \n",
    "            # Prepare fact_payroll DataFrame\n",
    "            fact_payroll_data = df_cleaned[['PayrollNumber', 'EmployeeID', 'AgencyID', 'TitleCode','FiscalYear', 'BaseSalary',\n",
    "                                            'RegularHours', 'RegularGrossPaid', 'OTHours', 'TotalOTPaid', 'TotalOtherPay',\n",
    "                                            'WorkLocationBorough']]\n",
    "            fact_payroll_df = pd.concat([\n",
    "                fact_payroll_df,\n",
    "                ensure_columns(fact_payroll_data, table_schemas['fact_payroll']).drop_duplicates()\n",
    "            ], ignore_index=True)\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing file {file_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Stage the data into the database\n",
    "    try:\n",
    "        stage_data(engine, dim_employee_df, 'dim_employee')\n",
    "        stage_data(engine, dim_agency_df, 'dim_agency')\n",
    "        stage_data(engine, dim_title_df, 'dim_title')\n",
    "        stage_data(engine, fact_payroll_df, 'fact_payroll')\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error staging data: {e}\")\n",
    "        return\n",
    "\n",
    "    # Calculate total rows staged\n",
    "    total_transactional_rows = len(fact_payroll_df)\n",
    "\n",
    "    # Update Prometheus metrics\n",
    "    rows_transformed.set(len(fact_payroll_df))\n",
    "    rows_staged.set(total_transactional_rows)\n",
    "\n",
    "    # Log success message with details\n",
    "    logging.info(\"Transactional data successfully transformed and staged.\")\n",
    "    logging.info(f\" - fact_payroll: {total_transactional_rows} rows\")\n",
    "    logging.info(f\"Total transactional data staged: {total_transactional_rows} rows\")\n",
    "    logging.info(\"All data successfully transformed and staged.\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    load_dotenv(override=True)\n",
    "    \n",
    "    master_files = ['EmpMaster.csv', 'TitleMaster.csv', 'AgencyMaster.csv']\n",
    "    payroll_files = ['nycpayroll_2021.csv','nycpayroll_2020.csv']\n",
    "\n",
    "    global table_schemas, attributes, s3_client, engine, s3_bucket, s3_prefix\n",
    "    table_schemas = {\n",
    "        'dim_employee': ['EmployeeID', 'FirstName', 'LastName', 'LeaveStatusasofJune30'],\n",
    "        'dim_agency': ['AgencyID', 'AgencyName', 'AgencyStartDate'],\n",
    "        'dim_title': ['TitleCode', 'TitleDescription'],\n",
    "        'fact_payroll': ['PayrollNumber', 'EmployeeID','AgencyID', 'TitleCode','FiscalYear', 'BaseSalary', 'RegularHours',\n",
    "                        'RegularGrossPaid', 'OTHours', 'TotalOTPaid', 'TotalOtherPay', 'WorkLocationBorough']\n",
    "    }\n",
    "\n",
    "    attributes = [\n",
    "        'FiscalYear', 'PayrollNumber', 'AgencyID', 'AgencyName', 'EmployeeID', 'LastName', 'FirstName',\n",
    "        'AgencyStartDate', 'WorkLocationBorough', 'TitleCode', 'TitleDescription', 'LeaveStatusasofJune30',\n",
    "        'BaseSalary', 'PayBasis', 'RegularHours', 'RegularGrossPaid', 'OTHours', 'TotalOTPaid', 'TotalOtherPay'\n",
    "    ]\n",
    "\n",
    "    s3_client = get_s3_client()\n",
    "    engine = redshift_engine()\n",
    "    setup_logging()\n",
    "\n",
    "    s3_bucket = os.getenv(\"s3_bucket\")\n",
    "    s3_prefix = os.getenv(\"s3_prefix\")\n",
    "\n",
    "    # Validate environment variables\n",
    "    if not s3_bucket or not s3_prefix:\n",
    "        logging.error(\"S3_BUCKET or S3_PREFIX environment variables are not set.\")\n",
    "        return\n",
    "\n",
    "    # Start processing\n",
    "    transform_transactional_data(payroll_files)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8115e104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:DBT run failed\n",
      "ERROR:root:STDERR:\n",
      "ERROR:root:\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['dbt', 'run']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     35\u001b[0m     logging\u001b[38;5;241m.\u001b[39mbasicConfig(level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mINFO)  \u001b[38;5;66;03m# Set up logging\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     \u001b[43mdbt_trigger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 32\u001b[0m, in \u001b[0;36mdbt_trigger\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTDERR:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m logging\u001b[38;5;241m.\u001b[39merror(e\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mdecode())\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m, in \u001b[0;36mdbt_trigger\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m dbt_project_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetcwd(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdbt\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# This will resolve to '/Users/villy/PycharmProjects/Payroll-Data-Integration-Platform/dbt'\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Run DBT commands from the specified directory\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdbt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrun\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdbt_project_dir\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set the working directory to the dbt project directory\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDBT run successful\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTDOUT:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/subprocess.py:528\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m     retcode \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[0;32m--> 528\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m    529\u001b[0m                                  output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr)\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process\u001b[38;5;241m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['dbt', 'run']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import logging\n",
    "import os\n",
    "\n",
    "def dbt_trigger():\n",
    "    \"\"\"\n",
    "    Trigger the DBT process for further transformations and loading data into the final warehouse.\n",
    "\n",
    "    This function assumes that the DBT environment is already set up and that the necessary\n",
    "    DBT commands will execute successfully.\n",
    "    \"\"\"\n",
    "    # Set the path to the dbt project directory\n",
    "    dbt_project_dir = os.path.join(os.getcwd(), 'dbt')\n",
    "\n",
    "    try:\n",
    "        # Run DBT commands from the specified directory\n",
    "        result = subprocess.run(\n",
    "            ['dbt', 'run'],\n",
    "            check=True,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            cwd=dbt_project_dir  # Set the working directory to the dbt project directory\n",
    "        )\n",
    "        \n",
    "        logging.info(\"DBT run successful\")\n",
    "        logging.info(\"STDOUT:\")\n",
    "        logging.info(result.stdout.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logging.error(\"DBT run failed\")\n",
    "        logging.error(\"STDERR:\")\n",
    "        logging.error(e.stderr.decode())\n",
    "        raise e\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO)  # Set up logging\n",
    "    dbt_trigger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "685975d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:DBT run failed\n",
      "ERROR:root:\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['dbt', 'run']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscripts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdbt_trigger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dbt_trigger\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdbt_trigger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/Payroll-Data-Integration-Platform/scripts/dbt_trigger.py:29\u001b[0m, in \u001b[0;36mdbt_trigger\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDBT run failed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m logging\u001b[38;5;241m.\u001b[39merror(e\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mdecode())\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/PycharmProjects/Payroll-Data-Integration-Platform/scripts/dbt_trigger.py:16\u001b[0m, in \u001b[0;36mdbt_trigger\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m dbt_project_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./dbt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Run DBT commands from the specified directory\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdbt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrun\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdbt_project_dir\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set the working directory\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDBT run successful\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(result\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mdecode())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/subprocess.py:528\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m     retcode \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[0;32m--> 528\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m    529\u001b[0m                                  output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr)\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process\u001b[38;5;241m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['dbt', 'run']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "from scripts.dbt_trigger import dbt_trigger\n",
    "\n",
    "dbt_trigger()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
